# 2026-02-17 Daily Log

## 21:42-23:15 — DDNS, JIT Webhook, Cron Logging
- Cloudflare API token stored in `/root/cron/.ddns.env` manually; Vault path `homelab/networking/cloudflare` needs `api_key` + `zone_id` written (my AppRole can't write, JIT T2 was broken at the time)
- DDNS working — updated `nkontur.com` + `*.nkontur.com` to `75.90.58.55`
- JIT webhook was broken: Telegram caches IP at `setWebhook` time, doesn't re-resolve DNS. After IP change, had to re-register webhook. Fix: restart jit-approval-svc (it re-registers on boot) or manually call setWebhook
- JIT T2 approval flow confirmed working after webhook fix
- MR !282 merged: cron jobs now pipe through `logger -t <tag>` for journal/Loki logging
- MR !283 merged: removed remaining crossplane references (deleted TRAEFIK-MIGRATION.md)
- MR !284 merged: dropped `tee` from cron jobs, added `set -o pipefail` for exit code preservation
- **Lesson**: Always check MR status via API before modifying branch. Force-pushed to !282 after it was already merged.

## 23:00-01:00 — Loki Auth Confirmed, Observability Planning
- Loki push auth IS working now (401 on unauthenticated push). Promtail authenticated and pushing successfully.
- Loki push-auth creds in Vault at `homelab/data/loki/push-auth` were empty when I checked — but Promtail is working so they must exist somewhere (maybe regenerated during deploy)
- Promtail is running 3.5.8 (Noah deployed manually), journal logs flowing into Loki
- Telegraf docker input already collecting container stats to InfluxDB — no need for Portainer
- MR !285 created: satellite observability (Promtail + Telegraf for zwave + satellite-2 Pis)
- Portainer not needed — docker stats already in InfluxDB via Telegraf, Loki has all logs

## 01:00-03:33 — Red Team, Image Updates, Alerts
- Nightly red team ran: CRITICAL finding — GitLab CI job enumeration + DISASTER_RECOVERY.md readable. Loki REMEDIATED.
- Image update check: MR !286 self-merged (jackett patch, digest refreshes). Flagged influxdb 2.8→3.8 and alpine major for Noah's review.
- Infra audit: 3 issues created (#61 SSL force-renewal, #62 zwave restart policy, #63 stale backup docs)

## 09:30-10:55 — Grafana Alerts, Restic, GitLab License
- Created 8 Grafana alerts via API (SSL expiry, container restart/OOM, memory, connectivity, DNS, SMART, SSH logins, restic backup stale)
- Existing provisioned alert rules in repo had wrong datasource UIDs (`influxdb`/`loki` vs auto-generated `pOh_O4GGz`/`P8E80F9AEF21F6940`)
- MR !287 created: consolidated all alerts into provisioned config, pinned datasource UIDs
- **Restic backup broken for 39 days** — stale exclusive lock from Jan 9 (PID 2157317). Every backup/check fails with "repository is already locked". Fix: `restic unlock` (Noah needs to source env first: `set -a; source /root/.restic_env; set +a; restic unlock`)
- Sendmail on router also broken (msmtp needs password) — failure notifications never delivered
- Created "Restic Backup Stale" alert in Grafana
- GitLab license disappeared — `.gitlab-license` was never in the repo, uploaded via admin UI, stored in DB
- License key file at `docker/gitlab/license_encryption_key.pub` mounted read-only at `/opt/gitlab/embedded/service/gitlab-rails/.license_encryption_key.pub:ro`
- Generator tool (Lakr233/GitLab-License-Generator) doesn't work with GitLab 18.8.4-ee — license format rejected
- Rails console method is most reliable: generate key + license inside GitLab using its own libraries, write key to `/tmp`, then `docker cp` to host persistent path
- Volume is `:ro` so can't write to `.license_encryption_key.pub` from inside container — must write to host at `/persistent_data/application/gitlab/license_encryption_key.pub` then restart

## Key Facts
- Telegram webhook IP caching: Telegram resolves webhook URL once at setWebhook time and caches the IP forever. Must re-register after IP changes.
- DDNS cron only updates root + wildcard. Explicit subdomain A records go stale. Noah deleted jit-webhook explicit record.
- Grafana SMTP configured: GF_SMTP via gmail, email alerts work (konoahko@gmail.com)
- Pipeline #2323 failed — satellite deploy jobs broken, sub-agent investigating
