apiVersion: 1

groups:
  # =========================================================================
  # Container Alerts
  # =========================================================================
  - orgId: 1
    name: container-alerts
    folder: General Alerting
    interval: 1m
    rules:
      - uid: container-health-failure
        title: Container Health Check Failed
        condition: B
        data:
          - refId: A
            relativeTimeRange: {from: 300, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r._measurement == "docker_container_health")
                  |> filter(fn: (r) => r._field == "failing_streak")
                  |> filter(fn: (r) => r.container_name != "audioserve" and r.container_name != "nextcloud")
                  |> filter(fn: (r) => r._value > 0)
                  |> group(columns: ["container_name"])
                  |> last()
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model:
              type: reduce
              expression: A
              reducer: last
              refId: B
              settings: {mode: dropNN}
        noDataState: OK
        execErrState: Alerting
        for: 10m
        annotations:
          summary: "Container {{ $labels.container_name }} health check failed"
        labels:
          severity: critical

      - uid: container-restart-loop
        title: Container Restart Loop
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 900, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -15m)
                  |> filter(fn: (r) => r._measurement == "docker_container_status")
                  |> filter(fn: (r) => r._field == "restart_count")
                  |> difference(nonNegative: true)
                  |> group(columns: ["container_name"])
                  |> sum()
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: dropNN}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [3]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 5m
        annotations:
          summary: "Container restarting repeatedly (>3 restarts in 15min)"
        labels:
          severity: critical

      - uid: container-oom-kill
        title: Container OOM Kill
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 300, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r._measurement == "docker_container_status")
                  |> filter(fn: (r) => r._field == "oomkilled")
                  |> last()
                  |> filter(fn: (r) => r._value == true)
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: dropNN}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [0]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 5m
        annotations:
          summary: "Container killed by OOM"
        labels:
          severity: critical

      - uid: critical-service-down
        title: Critical Service Down
        condition: B
        data:
          - refId: A
            relativeTimeRange: {from: 300, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r._measurement == "docker_container_status")
                  |> filter(fn: (r) => r._field == "exitcode")
                  |> filter(fn: (r) => r.container_name =~ /grafana|homeassistant|nginx|gitlab|influxdb|plex|bitwarden|vault|loki|promtail/)
                  |> last()
                  |> filter(fn: (r) => r._value != 0)
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: last, refId: B, settings: {mode: dropNN}}
        noDataState: OK
        execErrState: Alerting
        for: 10m
        annotations:
          summary: "Critical service {{ $labels.container_name }} has non-zero exit code"
        labels:
          severity: critical

  # =========================================================================
  # System Alerts
  # =========================================================================
  - orgId: 1
    name: system-alerts
    folder: General Alerting
    interval: 1m
    rules:
      - uid: high-memory-usage
        title: High Memory Usage (90%+)
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 300, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r._measurement == "mem")
                  |> filter(fn: (r) => r._field == "used_percent")
                  |> mean()
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: dropNN}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [90]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 5m
        annotations:
          summary: "System memory usage above 90%"
        labels:
          severity: warning

      - uid: high-cpu-temps
        title: CPU Temperature High (80°C+)
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 300, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r._measurement == "ipmi_sensor")
                  |> filter(fn: (r) => r._field == "value")
                  |> filter(fn: (r) => r.name == "cpu1_temp" or r.name == "cpu2_temp")
                  |> mean()
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: dropNN}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [80]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 5m
        annotations:
          summary: "CPU temperature above 80°C"
        labels:
          severity: warning

      - uid: disk-space-warning
        title: Disk Space Warning (80%+)
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 300, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r._measurement == "disk")
                  |> filter(fn: (r) => r._field == "used_percent")
                  |> filter(fn: (r) => r.device !~ /^loop/)
                  |> filter(fn: (r) => r.path !~ /^\/snap/)
                  |> group(columns: ["device", "path", "host"])
                  |> mean()
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: dropNN}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [80]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 10m
        annotations:
          summary: "Disk usage above 80% on {{ $labels.device }} ({{ $labels.path }})"
        labels:
          severity: warning

      - uid: disk-space-critical
        title: Disk Space Critical (90%+)
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 300, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r._measurement == "disk")
                  |> filter(fn: (r) => r._field == "used_percent")
                  |> filter(fn: (r) => r.device !~ /^loop/)
                  |> filter(fn: (r) => r.path !~ /^\/snap/)
                  |> group(columns: ["device", "path", "host"])
                  |> mean()
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: dropNN}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [90]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 5m
        annotations:
          summary: "CRITICAL: Disk usage above 90% on {{ $labels.device }} ({{ $labels.path }})"
        labels:
          severity: critical

      # DISABLED: smart_device measurement not available - enable when smartmontools Telegraf plugin is configured
      # - uid: smart-disk-error
      #   title: SMART Disk Error
      #   condition: C
      #   data:
      #     - refId: A
      #       relativeTimeRange: {from: 3600, to: 0}
      #       datasourceUid: influxdb
      #       model:
      #         query: |
      #           from(bucket: "metrics")
      #             |> range(start: -1h)
      #             |> filter(fn: (r) => r._measurement == "smart_device")
      #             |> filter(fn: (r) => r._field == "health_ok")
      #             |> last()
      #             |> filter(fn: (r) => r._value == false)
      #         refId: A
      #     - refId: B
      #       relativeTimeRange: {from: 0, to: 0}
      #       datasourceUid: __expr__
      #       model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: dropNN}}
      #     - refId: C
      #       relativeTimeRange: {from: 0, to: 0}
      #       datasourceUid: __expr__
      #       model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [-1]}}]}
      #   noDataState: OK
      #   execErrState: Alerting
      #   for: 0s
      #   annotations:
      #     summary: "SMART reports disk health issue"
      #   labels:
      #     severity: critical

  # =========================================================================
  # Network Alerts
  # =========================================================================
  - orgId: 1
    name: network-alerts
    folder: General Alerting
    interval: 1m
    rules:
      - uid: internet-connectivity-loss
        title: Internet Connectivity Loss
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 300, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r._measurement == "ping")
                  |> filter(fn: (r) => r._field == "result_code")
                  |> filter(fn: (r) => r._value != 0)
                  |> group(columns: ["url"])
                  |> count()
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: sum, refId: B, settings: {mode: dropNN}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [0]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 10m
        annotations:
          summary: "Internet connectivity loss — ping failures detected"
        labels:
          severity: critical

      - uid: dns-resolution-failure
        title: DNS Resolution Failure
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 300, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r._measurement == "dns_query")
                  |> filter(fn: (r) => r._field == "result_code")
                  |> filter(fn: (r) => r._value != 0)
                  |> count()
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: dropNN}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [2]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 10m
        annotations:
          summary: "DNS resolution failing"
        labels:
          severity: critical

  # =========================================================================
  # Certificate Alerts
  # =========================================================================
  - orgId: 1
    name: certificate-alerts
    folder: General Alerting
    interval: 6h
    rules:
      - uid: ssl-cert-expiry
        title: SSL Certificate Expiry (<14 days)
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 21600, to: 0}
            datasourceUid: influxdb
            model:
              query: |
                from(bucket: "metrics")
                  |> range(start: -6h)
                  |> filter(fn: (r) => r._measurement == "x509_cert")
                  |> filter(fn: (r) => r._field == "expiry")
                  |> last()
                  |> map(fn: (r) => ({r with _value: r._value / 3600.0}))
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: min, refId: B, settings: {mode: dropNN}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: lt, params: [336]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 10m
        annotations:
          summary: "SSL certificate expiring within 14 days"
        labels:
          severity: warning

  # =========================================================================
  # UPS / Hardware Alerts
  # =========================================================================
  - orgId: 1
    name: hardware-alerts
    folder: General Alerting
    interval: 1m
    rules: []
      # DISABLED: snmp measurement not available - enable when UPS SNMP monitoring is configured
      # - uid: ups-battery-low
      #   title: UPS Battery Low
      #   condition: C
      #   data:
      #     - refId: A
      #       relativeTimeRange: {from: 300, to: 0}
      #       datasourceUid: influxdb
      #       model:
      #         query: |
      #           from(bucket: "metrics")
      #             |> range(start: -5m)
      #             |> filter(fn: (r) => r._measurement == "snmp")
      #             |> filter(fn: (r) => r._field == "percent_charge")
      #             |> last()
      #         refId: A
      #     - refId: B
      #       relativeTimeRange: {from: 0, to: 0}
      #       datasourceUid: __expr__
      #       model: {type: reduce, expression: A, reducer: min, refId: B, settings: {mode: dropNN}}
      #     - refId: C
      #       relativeTimeRange: {from: 0, to: 0}
      #       datasourceUid: __expr__
      #       model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: lt, params: [50]}}]}
      #   noDataState: OK
      #   execErrState: Alerting
      #   for: 2m
      #   annotations:
      #     summary: "UPS battery charge below 50%"
      #   labels:
      #     severity: critical

      # DISABLED: snmp measurement not available - enable when UPS SNMP monitoring is configured
      # - uid: ups-high-load
      #   title: UPS High Load
      #   condition: C
      #   data:
      #     - refId: A
      #       relativeTimeRange: {from: 300, to: 0}
      #       datasourceUid: influxdb
      #       model:
      #         query: |
      #           from(bucket: "metrics")
      #             |> range(start: -5m)
      #             |> filter(fn: (r) => r._measurement == "snmp")
      #             |> filter(fn: (r) => r._field == "load")
      #             |> last()
      #         refId: A
      #     - refId: B
      #       relativeTimeRange: {from: 0, to: 0}
      #       datasourceUid: __expr__
      #       model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: dropNN}}
      #     - refId: C
      #       relativeTimeRange: {from: 0, to: 0}
      #       datasourceUid: __expr__
      #       model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [80]}}]}
      #   noDataState: OK
      #   execErrState: Alerting
      #   for: 5m
      #   annotations:
      #     summary: "UPS load above 80%"
      #   labels:
      #     severity: warning

  # =========================================================================
  # Backup Alerts
  # =========================================================================
  - orgId: 1
    name: backup-alerts
    folder: General Alerting
    interval: 30m
    rules:
      - uid: restic-backup-stale
        title: Restic Backup Stale (48h)
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 172800, to: 0}
            datasourceUid: loki
            model:
              expr: 'count_over_time({unit="restic-backup.service"} |= "snapshot" |= "saved" [48h])'
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: replaceNN, replaceWithValue: 0}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: lt, params: [1]}}]}
        noDataState: Alerting
        execErrState: Alerting
        for: 5m
        annotations:
          summary: "No successful restic backup in 48 hours"
          description: "Check: journalctl -u restic-backup.service --since '48 hours ago'"
        labels:
          severity: critical

      - uid: restic-backup-error
        title: Restic Backup Error
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 3600, to: 0}
            datasourceUid: loki
            model:
              expr: 'count_over_time({unit="restic-backup.service"} |~ "(?i)(fatal|unable to create lock)" [1h])'
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: replaceNN, replaceWithValue: 0}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [0]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 0s
        annotations:
          summary: "Restic backup failed with errors"
          description: "Check: journalctl -u restic-backup.service --since '1 hour ago'"
        labels:
          severity: critical

  # =========================================================================
  # Security Alerts
  # =========================================================================
  - orgId: 1
    name: security-alerts
    folder: General Alerting
    interval: 5m
    rules:
      - uid: failed-ssh-logins
        title: Failed SSH Logins
        condition: C
        data:
          - refId: A
            relativeTimeRange: {from: 900, to: 0}
            datasourceUid: loki
            model:
              expr: 'count_over_time({unit="ssh.service"} |= "Failed password" [15m])'
              refId: A
          - refId: B
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: reduce, expression: A, reducer: max, refId: B, settings: {mode: replaceNN, replaceWithValue: 0}}
          - refId: C
            relativeTimeRange: {from: 0, to: 0}
            datasourceUid: __expr__
            model: {type: threshold, expression: B, refId: C, conditions: [{evaluator: {type: gt, params: [5]}}]}
        noDataState: OK
        execErrState: Alerting
        for: 0s
        annotations:
          summary: "Multiple failed SSH login attempts (>5 in 15min)"
        labels:
          severity: warning
