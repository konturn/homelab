apiVersion: 1

groups:
  - orgId: 1
    name: infrastructure
    folder: Infrastructure Alerts
    interval: 1m
    rules:
      - uid: container-health-failure
        title: Container Health Check Failed
        condition: B
        data:
          - refId: A
            queryType: ""
            relativeTimeRange:
              from: 300
              to: 0
            model:
              datasource:
                type: influxdb
                uid: influxdb
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r["_measurement"] == "docker_container_health")
                  |> filter(fn: (r) => r["_field"] == "health_status")
                  |> filter(fn: (r) => r["_value"] != "healthy")
                  |> group(columns: ["container_name"])
                  |> last()
              refId: A
          - refId: B
            queryType: ""
            relativeTimeRange:
              from: 0
              to: 0
            model:
              conditions:
                - evaluator:
                    params: [0]
                    type: gt
                  operator:
                    type: and
                  query:
                    params: [A]
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
        noDataState: NoData
        execErrState: Alerting
        for: 2m
        annotations:
          summary: "Container {{ $labels.container_name }} health check failed"
          description: "Container {{ $labels.container_name }} has been reporting unhealthy status for more than 2 minutes. This indicates the container service may be down or malfunctioning."
        labels:
          severity: critical
          team: infrastructure
          alertname: ContainerHealthFailure

      - uid: high-disk-usage-warning
        title: Disk Space Warning (80%+)
        condition: B
        data:
          - refId: A
            queryType: ""
            relativeTimeRange:
              from: 300
              to: 0
            model:
              datasource:
                type: influxdb
                uid: influxdb
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r["_measurement"] == "disk")
                  |> filter(fn: (r) => r["_field"] == "used_percent")
                  |> filter(fn: (r) => r["_value"] >= 80.0)
                  |> group(columns: ["device", "path"])
                  |> mean()
              refId: A
          - refId: B
            queryType: ""
            relativeTimeRange:
              from: 0
              to: 0
            model:
              conditions:
                - evaluator:
                    params: [80]
                    type: gt
                  operator:
                    type: and
                  query:
                    params: [A]
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
        noDataState: NoData
        execErrState: NoData
        for: 5m
        annotations:
          summary: "Disk space warning on {{ $labels.device }} ({{ $labels.path }})"
          description: "Disk usage on {{ $labels.device }} mounted at {{ $labels.path }} is {{ $value }}%, which exceeds the 80% warning threshold. Consider freeing up space or extending storage."
        labels:
          severity: warning
          team: infrastructure
          alertname: DiskSpaceWarning

      - uid: high-disk-usage-critical
        title: Disk Space Critical (90%+)
        condition: B
        data:
          - refId: A
            queryType: ""
            relativeTimeRange:
              from: 300
              to: 0
            model:
              datasource:
                type: influxdb
                uid: influxdb
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r["_measurement"] == "disk")
                  |> filter(fn: (r) => r["_field"] == "used_percent")
                  |> filter(fn: (r) => r["_value"] >= 90.0)
                  |> group(columns: ["device", "path"])
                  |> mean()
              refId: A
          - refId: B
            queryType: ""
            relativeTimeRange:
              from: 0
              to: 0
            model:
              conditions:
                - evaluator:
                    params: [90]
                    type: gt
                  operator:
                    type: and
                  query:
                    params: [A]
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
        noDataState: NoData
        execErrState: NoData
        for: 2m
        annotations:
          summary: "CRITICAL: Disk space on {{ $labels.device }} ({{ $labels.path }}) at {{ $value }}%"
          description: "Disk usage on {{ $labels.device }} mounted at {{ $labels.path }} is {{ $value }}%, which exceeds the 90% critical threshold. Immediate action required to prevent system issues."
        labels:
          severity: critical
          team: infrastructure
          alertname: DiskSpaceCritical

      - uid: high-memory-usage
        title: High Memory Usage (85%+)
        condition: B
        data:
          - refId: A
            queryType: ""
            relativeTimeRange:
              from: 600
              to: 0
            model:
              datasource:
                type: influxdb
                uid: influxdb
              query: |
                from(bucket: "metrics")
                  |> range(start: -10m)
                  |> filter(fn: (r) => r["_measurement"] == "mem")
                  |> filter(fn: (r) => r["_field"] == "used_percent")
                  |> filter(fn: (r) => r["_value"] >= 85.0)
                  |> mean()
              refId: A
          - refId: B
            queryType: ""
            relativeTimeRange:
              from: 0
              to: 0
            model:
              conditions:
                - evaluator:
                    params: [85]
                    type: gt
                  operator:
                    type: and
                  query:
                    params: [A]
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
        noDataState: NoData
        execErrState: NoData
        for: 10m
        annotations:
          summary: "High memory usage detected: {{ $value }}%"
          description: "System memory usage has been at {{ $value }}% for more than 10 minutes, which exceeds the 85% threshold. This may impact system performance and stability."
        labels:
          severity: warning
          team: infrastructure
          alertname: HighMemoryUsage

      - uid: service-downtime-critical
        title: Critical Service Down
        condition: B
        data:
          - refId: A
            queryType: ""
            relativeTimeRange:
              from: 300
              to: 0
            model:
              datasource:
                type: influxdb
                uid: influxdb
              query: |
                from(bucket: "metrics")
                  |> range(start: -5m)
                  |> filter(fn: (r) => r["_measurement"] == "docker_container_status")
                  |> filter(fn: (r) => r["_field"] == "oom_killed" or r["_field"] == "status")
                  |> filter(fn: (r) => r["container_name"] =~ /grafana|homeassistant|nginx|gitlab|influxdb|plex|bitwarden|vault|loki/)
                  |> filter(fn: (r) => r["_value"] != "running" or r["oom_killed"] == 1)
                  |> group(columns: ["container_name"])
                  |> last()
              refId: A
          - refId: B
            queryType: ""
            relativeTimeRange:
              from: 0
              to: 0
            model:
              conditions:
                - evaluator:
                    params: [0]
                    type: gt
                  operator:
                    type: and
                  query:
                    params: [A]
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
        noDataState: NoData
        execErrState: Alerting
        for: 1m
        annotations:
          summary: "Critical service {{ $labels.container_name }} is down"
          description: "Critical homelab service {{ $labels.container_name }} has stopped running or was OOM killed. This service is essential for homelab operations and requires immediate attention."
        labels:
          severity: critical
          team: infrastructure
          alertname: ServiceDowntime

      - uid: loki-log-errors
        title: High Error Rate in Logs
        condition: B
        data:
          - refId: A
            queryType: ""
            relativeTimeRange:
              from: 300
              to: 0
            model:
              datasource:
                type: loki
                uid: loki
              expr: 'rate({job=~".+"} |~ "(?i)(error|fatal|exception|panic)" [5m])'
              refId: A
          - refId: B
            queryType: ""
            relativeTimeRange:
              from: 0
              to: 0
            model:
              conditions:
                - evaluator:
                    params: [0.1]
                    type: gt
                  operator:
                    type: and
                  query:
                    params: [A]
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              intervalMs: 1000
              maxDataPoints: 43200
              reducer: last
              refId: B
              type: reduce
        noDataState: NoData
        execErrState: NoData
        for: 5m
        annotations:
          summary: "High error rate detected in logs"
          description: "Error rate in application logs has exceeded 0.1 errors per second over the last 5 minutes. This may indicate service issues requiring investigation."
        labels:
          severity: warning
          team: infrastructure
          alertname: LogErrorRate