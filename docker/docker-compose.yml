volumes:
  nextcloud_db:
  wordpress_db:
  ombi:
  fifos:
  # registry volume removed - using GitLab registry instead
  mosquitto:
  vault_data:
  nextcloud:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /mpool/nextcloud/nextcloud
  influxdb-storage:
  grafana-storage:
  otp_data:
  nextcloud_aio_mastercontainer:
    name: nextcloud_aio_mastercontainer
networks:
  mgmt:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.4.0.0/16
            ip_range: 10.4.0.0/18
            aux_addresses:
              shim4: 10.4.0.2
    driver_opts:
      parent: bond0.4
  iot:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.6.0.0/16
            ip_range: 10.6.0.0/18
            aux_addresses:
              shim6: 10.6.0.2
    driver_opts:
      parent: bond0.6
  external:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.2.0.0/16
            ip_range: 10.2.0.0/18
            aux_addresses:
              shim3: 10.2.0.2
    driver_opts:
      parent: bond0.2
  internal:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.3.0.0/16
            ip_range: 10.3.0.0/18
            aux_addresses:
              shim2: 10.3.0.2
    driver_opts:
      parent: bond0.3
  guest:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.5.0.0/16
            ip_range: 10.5.0.0/18
            aux_addresses:
              shim2: 10.5.0.2
    driver_opts:
      parent: bond0.5
  wordpress_internal:
    driver: bridge
  nextcloud_internal:
    driver: bridge
  jit-bridge:
    driver: bridge
    internal: true
  moltbot-nginx-bridge:
    driver: bridge
    internal: true
  docker-proxy:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.30.0.0/24
services:
  # Docker socket proxy - filtered access for CI builds
  # Only allows build/image operations, blocks exec/start/stop
  docker-socket-proxy:
    container_name: docker-socket-proxy
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: tecnativa/docker-socket-proxy:0.3@sha256:1f3a6f303320723d199d2316a3e82b2e2685d86c275d5e3deeaf182573b47476
    restart: unless-stopped
    networks:
      docker-proxy:
    deploy:
      resources:
        limits:
          memory: 128M
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      # Allow build operations
      - BUILD=1
      - IMAGES=1
      - COMMIT=1
      # Allow auth for registry push
      - AUTH=1
      # Deny dangerous operations
      - CONTAINERS=0
      - EXEC=0
      - NETWORKS=0
      - VOLUMES=0
      - POST=1  # Needed for build/push
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:2375/_ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  plex:
    container_name: plex
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "32400/tcp"
    image: plexinc/pms-docker:1.43.0.10467-2b1ba6e69@sha256:d38767ec3b948e860bcf1abfe16512bd638e6ac01230f5b65b64e2ef09052267
    networks:
      - external
      - internal
    restart: unless-stopped
    # GPU disabled until NVML driver/library mismatch is fixed
    # runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 2G
    environment:
      - TZ=America/New_York
      # - NVIDIA_VISIBLE_DEVICES=all
      # - NVIDIA_DRIVER_CAPABILITIES=compute,video,utility
    volumes:
      - /mpool/plex/config:/config
      - /mpool/plex/transcode:/transcode
      - /mpool/plex:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:32400/identity"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
  pihole:
    container_name: pihole
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN            # pihole user/group setup changes file ownership
      - DAC_OVERRIDE     # access config files across pihole/root users
      - FOWNER           # chmod on gravity.db during gravity updates
      - NET_ADMIN        # DHCP server and network interface management
      - NET_BIND_SERVICE # DNS listener on port 53
      - NET_RAW          # DHCP raw socket access
      - SETGID           # drop to pihole group
      - SETUID           # drop to pihole user
    image: pihole/pihole:2025.11.1@sha256:848cf5af61397e6976e2fc356aa00aaa3d3a53b2ac90a956675779ff1f4bdf34
    ports:
      - "443:80"
    networks:
      internal:
        ipv4_address: {{ pihole_ip }}
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      TZ: America/New_York
      WEBPASSWORD: {{ vault_pihole_password | default(lookup('env', 'PIHOLE_PASSWORD')) | trim }}
      DNSMASQ_USER: root
    volumes:
      - {{ docker_persistent_data_path }}/pihole/conf:/etc/pihole
      - {{ docker_persistent_data_path }}/pihole/dnsmasq:/etc/dnsmasq.d
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "dig", "+short", "+norecurse", "+retry=0", "@127.0.0.1", "pi.hole"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  iperf3:
    container_name: iperf3
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    tmpfs:
      - /tmp
    image: networkstatic/iperf3:latest@sha256:07dcca91c0e47d82d83d91de8c3d46b840eef4180499456b4fa8d6dadb46b6c8
    ports:
      - "5201/tcp"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 128M
    command:
      - "-s"
    restart: unless-stopped
  nginx: 
    image: nginx:1.29.5-alpine3.23-perl@sha256:db0a3121225ffbc32fa36cc7a02c6c11002fec0c7fd2596ba204a811cc4a71dc
    container_name: nginx
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # nginx master runs as root, workers drop to nginx user; binds to port 80
    cap_add:
      - CHOWN            # manage log/pid file ownership
      - DAC_OVERRIDE     # read config and write logs across users
      - NET_BIND_SERVICE # bind to port 80
      - SETGID           # worker processes drop to nginx group
      - SETUID           # worker processes drop to nginx user
    read_only: true
    tmpfs:
      - /tmp
      - /run
      - /var/cache/nginx
    networks:
      external:
        ipv4_address: {{ nginx_ip }}
      jit-bridge:
      moltbot-nginx-bridge:
        ipv4_address: 172.30.0.10
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pgrep nginx > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    volumes:
      - {{ docker_persistent_data_path }}/nginx/conf:/etc/nginx
      - {{ docker_persistent_data_path }}/nginx/webroot:/data/webroot
      - /var/log/nginx:/data/log
      - {{ docker_persistent_data_path }}/certs:/data/certs
  lab_nginx: 
    image: nginx:1.29.5-alpine3.23-perl@sha256:db0a3121225ffbc32fa36cc7a02c6c11002fec0c7fd2596ba204a811cc4a71dc
    container_name: lab_nginx
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as nginx above
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - NET_BIND_SERVICE
      - SETGID
      - SETUID
    read_only: true
    tmpfs:
      - /tmp
      - /run
      - /var/cache/nginx
    networks:
      internal:
        ipv4_address: {{ lab_nginx_ip }}
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pgrep nginx > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    volumes:
      - {{ docker_persistent_data_path }}/lab_nginx/conf:/etc/nginx
      - /var/log/lab_nginx:/data/log
      - {{ docker_persistent_data_path }}/certs:/data/certs
  iot_nginx: 
    image: nginx:1.29.5-alpine3.23-perl@sha256:db0a3121225ffbc32fa36cc7a02c6c11002fec0c7fd2596ba204a811cc4a71dc
    container_name: iot_nginx
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as nginx above
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - NET_BIND_SERVICE
      - SETGID
      - SETUID
    read_only: true
    tmpfs:
      - /tmp
      - /run
      - /var/cache/nginx
    networks:
      iot:
        ipv4_address: {{ iot_nginx_ip }}
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pgrep nginx > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    volumes:
      - {{ docker_persistent_data_path }}/iot_nginx/conf:/etc/nginx
      - /var/log/iot_nginx:/data/log
      - {{ docker_persistent_data_path }}/certs:/data/certs
  ombi:
    image: linuxserver/ombi:4.53.4@sha256:fcad7b900ab2bc910f8271435953d99fd8dd4a1d877d09e478c9183bd4591193
    container_name: ombi
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    networks:
      - external
      - internal
    deploy:
      resources:
        limits:
          memory: 768M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
      - BASE_URL=/plex-requests
    volumes:
      - ombi:/config
      - /etc/ssl/certs:/etc/ssl/certs
      - /etc/ssl/private:/etc/ssl/private
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3579/api/v1/Status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  nzbget:
    image: linuxserver/nzbget:26.0.20260206@sha256:c96d3a5c7876d183b82ee164dd2f8a56d85ee533cd41113dfb9dca6408f191a8
    container_name: nzbget
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "6789:6789"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/nzbget:/config
      - /mpool/plex/Movies:/movies
      - /mpool/samba_share/nfs:/remote
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6789"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  radarr:
    image: linuxserver/radarr:6.0.4@sha256:ba2693dd704b84eb0b404d40b3902bd3e62a1768dc5ee0d89b1f1d7cd51a66eb
    container_name: radarr
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "443:7878"
    networks:
      - internal
    # Memory increased from 1G to 2G: library has 5300+ movies (28MB API payload).
    # 1G limit causes memory pressure during large serialization + concurrent tasks.
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/radarr:/config
      - /mpool/plex/Movies:/movies
      - /mpool/samba_share/nfs:/remote
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7878/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  jackett:
    image: linuxserver/jackett:0.24.1098@sha256:14eccea2966811ed8bfa5e635b2de965d22e5ec43da4f7fc15212ababd65ad86
    container_name: jackett
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "443:9117"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/jackett:/config
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9117/UI/Login"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  sonarr:
    image: linuxserver/sonarr:4.0.16@sha256:6f73bbba33391a338e20d836a60c86beaf2a865a89b706b339fc8cb0b8ce1559
    container_name: sonarr
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "443:8989"
    networks:
    - internal
    # 308K history records — Sonarr benefits from memory headroom for DB operations.
    # Added reservation to prevent OOM kills during heavy queries.
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/sonarr:/config
      - /mpool/plex/TV:/tv
      - /mpool/samba_share/nfs:/remote
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8989/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  booksonic:
    image: izderadicka/audioserve:latest@sha256:c3609321701765671cae121fc0f61db122e8c124643c04770fbc9326c74b18e3
    container_name: audioserve
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ports:
      - "443:3000"
    networks:
      - external
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
      - AUDIOSERVE_SHARED_SECRET={{ vault_audioserve_secret | default(lookup('env', 'AUDIOSERVE_SECRET')) | trim }}
      - VIRTUAL_HOST=audioserve.nkontur.com
      - LETSENCRYPT_HOST=audioserve.nkontur.com
    command: /audiobooks
    volumes:
      - /mpool/audioserve/audiobooks:/audiobooks
      - /mpool/audioserve/data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  influxdb:
    container_name: influxdb
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Entrypoint chowns data dirs and switches to influxdb user
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETGID
      - SETUID
    ports:
      - "443:8086"
    networks:
      internal:
        ipv4_address: {{ influxdb_ip }}
    image: influxdb:2.8.0@sha256:8e911da5f7b482230e61fe4bad9af0697d97a75e087f19f5f0ddfee62c2bd686
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 4G
    volumes:
        - influxdb-storage:/var/lib/influxdb2
        - {{ docker_persistent_data_path }}/influxdb/config:/etc/influxdb2
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD={{ vault_influxdb_password | default(lookup('env', 'INFLUXDB_PASSWORD')) | trim }}
      - DOCKER_INFLUXDB_INIT_ORG=homelab
      - DOCKER_INFLUXDB_INIT_BUCKET=metrics
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN={{ vault_influxdb_admin_token | default(lookup('env', 'INFLUXDB_ADMIN_TOKEN')) | trim }}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8086/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  grafana:
    container_name: grafana
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: grafana/grafana:12.4.0-21693836646-ubuntu@sha256:ba93c9d192e58b23e064c7f501d453426ccf4a85065bf25b705ab1e98602bfb1
    ports:
      - "443:3000"
    networks:
      - internal
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G
    volumes:
      - grafana-storage:/var/lib/grafana
      - {{ docker_persistent_data_path }}/grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD={{ vault_grafana_admin_password | default(lookup('env', 'GRAFANA_ADMIN_PASSWORD')) | trim }}
      - GF_SERVER_ROOT_URL=https://grafana.lab.nkontur.com
      - GF_USERS_ALLOW_SIGN_UP=false
      - INFLUXDB_TOKEN={{ vault_influxdb_admin_token | default(lookup('env', 'INFLUXDB_ADMIN_TOKEN')) | trim }}
      # SMTP for email alerts
      - GF_SMTP_ENABLED=true
      - GF_SMTP_HOST=smtp.gmail.com:587
      - GF_SMTP_USER=konoahko@gmail.com
      - GF_SMTP_PASSWORD={{ vault_grafana_smtp_password | default(lookup('env', 'GRAFANA_SMTP_PASSWORD')) | trim }}
      - GF_SMTP_FROM_ADDRESS=konoahko@gmail.com
      - GF_SMTP_FROM_NAME=Grafana Homelab
      - GF_SMTP_STARTTLS_POLICY=MandatoryStartTLS
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  loki:
    container_name: loki
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: grafana/loki:3.6@sha256:847c287ada0e12603910589f42038c5cdaaad04e248bd1dc6c6e0920a235f427
    networks:
      internal:
        ipv4_address: {{ loki_ip }}
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
    volumes:
      - {{ docker_persistent_data_path }}/loki/data:/loki
      - {{ docker_persistent_data_path }}/loki/local-config.yaml:/etc/loki/local-config.yaml:ro
    command: -config.file=/etc/loki/local-config.yaml
    # Loki uses default json-file logging to avoid circular dependency
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      disable: true
  promtail:
    image: grafana/promtail:3.6@sha256:6140a3158fddd7ef2276908bd321ad5265ee6ad2abd9fb3c937394827df3ea4d
    container_name: promtail
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    restart: unless-stopped
    volumes:
      - /var/log:/var/log:ro
      - /run/log/journal:/run/log/journal:ro
      - /etc/machine-id:/etc/machine-id:ro
      - {{ docker_persistent_data_path }}/promtail:/tmp/positions
      - {{ docker_persistent_data_path }}/promtail/config.yml:/etc/promtail/config.yml:ro
      - {{ docker_persistent_data_path }}/vault/logs:/vault/logs:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      internal:
        ipv4_address: {{ promtail_ip }}
    mem_limit: 128m
    cpus: 0.25
    # Use json-file logging to avoid circular dependency (same as Loki)
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      disable: true
  gitlab:
    container_name: gitlab
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Omnibus GitLab runs ~10 internal services (postgres, redis, nginx, puma, etc.)
    # each as separate users, requiring broad privilege management
    cap_add:
      - CHOWN            # data dir ownership across service users (git, postgres, redis)
      - DAC_OVERRIDE     # cross-user file access for internal services
      - FOWNER           # chmod/chown across service users
      - FSETID           # preserve setgid bits on shared directories
      - KILL             # signal management for internal service supervision (runsvdir)
      - NET_BIND_SERVICE # bind to ports 80 and 22
      - SETGID           # switch between internal service groups
      - SETUID           # switch between internal service users
      - SYS_CHROOT       # PostgreSQL and other services use chroot
      - SYS_RESOURCE     # set ulimits for internal services (runsvdir-start)
    ports:
      - "22/tcp"
      - "443:80"
    networks:
      - internal
    image: 'gitlab/gitlab-ee:18.6.6-ee.0@sha256:13d051c50d219b842485dd513dabc8c22d5f707bc44119433bec182e911ff243'
    restart: unless-stopped
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      nproc: 65536
    deploy:
      resources:
        limits:
          memory: 14G
        reservations:
          memory: 6G
    volumes:
      - '{{ docker_persistent_data_path }}/gitlab/config:/etc/gitlab'
      - '{{ docker_persistent_data_path }}/gitlab/logs:/var/log/gitlab'
      - '{{ docker_persistent_data_path }}/gitlab/data:/var/opt/gitlab'
      - '{{ docker_persistent_data_path }}/gitlab/license_encryption_key.pub:/opt/gitlab/embedded/service/gitlab-rails/.license_encryption_key.pub:ro'
    environment:
      MALLOC_CONF: 'dirty_decay_ms:1000,muzzy_decay_ms:1000'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/-/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 300s

  # HashiCorp Vault - Secrets Management
  # Initial setup uses file storage; will be configured for GitLab JWT auth in future MR
  vault:
    container_name: vault
    image: hashicorp/vault:1.21@sha256:a54f6229f8416dd5514020ecd06f514664c0cb21776b6686a3d26af21a79b1b1
    restart: unless-stopped
    networks:
      internal:
        ipv4_address: {{ vault_ip }}
    ports:
      - "8200:8200"
    deploy:
      resources:
        limits:
          memory: 768M
    cap_drop:
      - ALL
    cap_add:
      - IPC_LOCK    # Lock memory to prevent secrets from being swapped
      - CHOWN       # File ownership changes during startup
      - DAC_OVERRIDE # Access files regardless of permissions
      - FOWNER      # Modify file ownership metadata
      - SETUID      # Process UID changes
      - SETGID      # Process GID changes
    security_opt:
      - no-new-privileges:true
    volumes:
      - vault_data:/vault/file
      - '{{ docker_persistent_data_path }}/vault/config:/vault/config:ro'
      - '{{ docker_persistent_data_path }}/certs:/vault/certs:ro'
      - '{{ docker_persistent_data_path }}/vault/scripts/auto-unseal.sh:/vault/scripts/auto-unseal.sh:ro'
      - '{{ docker_persistent_data_path }}/vault/unseal:/vault/unseal:ro'
      - '{{ docker_persistent_data_path }}/vault/logs:/vault/logs'
    environment:
      - VAULT_ADDR=https://127.0.0.1:8200
      - VAULT_API_ADDR=https://{{ vault_ip }}:8200
      - VAULT_CACERT=/vault/certs/nkontur.com/live/iot.lab.nkontur.com-0003/chain.pem
      # Skip TLS verify for healthcheck using loopback
      - VAULT_SKIP_VERIFY=true
      # Path to unseal keys file inside the container
      - VAULT_UNSEAL_KEYS_FILE=/vault/unseal/unseal-keys
    entrypoint: /bin/sh
    command: ["/vault/scripts/auto-unseal.sh"]
    healthcheck:
      test: ["CMD", "sh", "-c", "vault status -address=https://127.0.0.1:8200; [ $? -le 2 ]"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # JIT Approval Service - Credential brokering with Telegram approval flow
  # Receives credential requests from moltbot, sends approval prompts via Telegram,
  # and issues short-lived Vault tokens on approval.
  jit-approval-svc:
    image: gitlab-registry.lab.nkontur.com:443/root/homelab/jit-approval-svc:latest
    container_name: jit-approval-svc
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    read_only: true
    cap_drop:
      - ALL
    tmpfs:
      - /tmp
    networks:
      internal:
        ipv4_address: {{ jit_approval_svc_ip }}
      jit-bridge:
    deploy:
      resources:
        limits:
          memory: 128M
    environment:
      - VAULT_ADDR=https://vault.lab.nkontur.com:8200
      - VAULT_ROLE_ID={{ vault_jit_approle_role_id }}
      - VAULT_SECRET_ID={{ vault_jit_approle_secret_id }}
      - TELEGRAM_BOT_TOKEN={{ vault_jit_telegram_bot_token }}
      - TELEGRAM_CHAT_ID=8531859108
      - TELEGRAM_WEBHOOK_SECRET={{ vault_jit_telegram_webhook_secret }}
      - LISTEN_ADDR=:8080
      - REQUEST_TIMEOUT=300
      - ALLOWED_REQUESTERS=prometheus
      - JIT_API_KEY={{ vault_jit_api_key }}
      - GITLAB_URL=https://gitlab.lab.nkontur.com
      - GITLAB_ADMIN_TOKEN={{ vault_jit_gitlab_admin_token }}
      - SSH_VAULT_PATH=ssh-client-signer
      - TAILSCALE_API_URL=https://api.tailscale.com
      - TELEGRAM_WEBHOOK_URL=https://jit-webhook.nkontur.com/telegram/webhook
    depends_on:
      vault:
        condition: service_healthy
    logging:
      driver: loki
      options:
        loki-url: "http://{{ loki_ip }}:3100/loki/api/v1/push"
        labels: "service=jit-approval-svc"
        loki-retries: "5"
        loki-batch-size: "400"
        max-buffer-size: "4m"
        mode: "non-blocking"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  deluge:
    container_name: deluge
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "443:8112"
    image: 'lscr.io/linuxserver/deluge:latest@sha256:2392a75ce975de1b50c03344b05a858c5da36c1c41ba1f61a5fe6198ea88ef18'
    restart: unless-stopped
    networks:
      internal:
        ipv4_address: {{ deluge_ip }}
    # Memory limit set to 2G after reducing libtorrent disk cache.
    # Default cache_size (512 pieces) causes 28GB+ RAM usage with large torrents.
    # Reduced to 128 pieces via core.conf — see configure-docker ansible role.
    deploy:
      resources:
        limits:
          memory: 2G
    volumes:
      - '{{ docker_persistent_data_path }}/deluge:/config'
      - '/mpool/samba_share/nfs:/downloads'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8112"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  bitwarden:
    container_name: bitwarden
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ports:
      - "443:80"
    image: 'vaultwarden/server:1.35.3-alpine@sha256:c40957876ec13c1cb0d2b08f86e8f738e6d06f7460bad9cdae216cded174c10d'
    restart: unless-stopped
    networks:
      - external
    deploy:
      resources:
        limits:
          memory: 512M
    volumes:
      - '{{ docker_persistent_data_path }}/bitwarden/data/:/data/'
    env_file:
      - '{{ docker_persistent_data_path }}/bitwarden/global.override.env'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/alive"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  blog:
    image: wordpress:6.9.1-php8.3-apache@sha256:0324d403512533271f85831518d0d8618ae794f8d50e40103cbac473119188af
    container_name: wordpress
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ports:
      - "443:80"
    networks:
      - external
      - wordpress_internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      WORDPRESS_DB_HOST: wordpress_db
      WORDPRESS_DB_USER: wordpress
      WORDPRESS_DB_PASSWORD: {{ vault_wordpress_db_password | default(lookup('env', 'WORDPRESS_DB_PASSWORD')) | trim }}
      WORDPRESS_DB_NAME: wordpress
    volumes:
      - {{ docker_persistent_data_path }}/wordpress/html:/var/www/html
      - {{ docker_persistent_data_path }}/wordpress/config:/etc/wordpress
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  wordpress_db:
    image: mysql:9.6.0-oraclelinux9@sha256:db32c8ec843c042a728efb0ac7aa814d6f010eaac8923e20ae0a849d09c5baf8
    container_name: wordpress_db
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    networks:
      - wordpress_internal
    restart: unless-stopped
    # MySQL entrypoint chowns data dir and drops to mysql user
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETGID
      - SETUID
      - SYS_NICE         # MySQL IO thread scheduling priority
    deploy:
      resources:
        limits:
          memory: 768M
    environment:
      MYSQL_DATABASE: wordpress
      MYSQL_USER: wordpress
      MYSQL_PASSWORD: {{ vault_wordpress_db_password | default(lookup('env', 'WORDPRESS_DB_PASSWORD')) | trim }}
      MYSQL_ROOT_PASSWORD: {{ vault_wordpress_db_password | default(lookup('env', 'WORDPRESS_DB_PASSWORD')) | trim }}
    volumes:
      - wordpress_db:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  diagram:
    container_name: diagram
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ports:
      - "443:8080"
    networks:
      internal:
    image: jgraph/drawio:29.3.6@sha256:e0951391432a0d8bc587d649107563166a2e52c8648e3c809429a9f56da6f386
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 768M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  homeassistant:
    container_name: homeassistant
    networks:
      - internal
      - iot
      - external
    ports:
      - "443:8123"
    devices: 
      - "/dev/serial/by-id/usb-Prolific_Technology_Inc._USB-Serial_Controller_ETDRb11A920-if00-port0:/dev/ttyUSB0"
      - "/dev/serial/by-id/usb-Prolific_Technology_Inc._USB-Serial_Controller-if00-port0:/dev/ttyUSB1"
    image: "homeassistant/home-assistant:2026.2@sha256:17441c45ba14560b4ef727ee06aac4d605cf0dc0625fc4f2e043cb2551d72749"
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    volumes:
      - "{{ docker_persistent_data_path }}/homeassistant:/config"
      - /etc/localtime:/etc/localtime:ro
    command: ["/bin/bash", "-c", "ip route del default; ip route add default via 10.3.0.1 dev eth1; /init"]
    cap_drop:
      - ALL
    cap_add:
      # Required: the entrypoint command runs `ip route del default; ip route add default via 10.3.0.1 dev eth1`
      # to override the default route so HA traffic goes through the internal VLAN (10.3.x.x) instead of
      # whatever Docker assigns. Without NET_ADMIN, the ip route commands fail and HA gets wrong default gateway.
      - NET_ADMIN
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8123/api/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
  snapserver:
    container_name: snapserver
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    hostname: snapserver
    networks:
      iot:
        ipv4_address: {{ snapserver_address }}
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapcast:latest"
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - "{{ docker_persistent_data_path }}/snapserver/snapserver.conf:/etc/snapserver.conf"
      - "{{ docker_persistent_data_path }}/snapserver/server.json:/root/.config/snapserver/server.json"
      - "/tmp/snapfifo:/tmp/snapfifo"
      - "fifos:/tmp/fifo"
    restart: unless-stopped
    devices:
      - "/dev/snd"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:1780"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  snapclient_office:
    container_name: snapclient_office
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Audio clients need user switching and real-time scheduling for playback
    cap_add:
      - SETGID    # drop to audio group
      - SETUID    # drop to audio user
      - SYS_NICE  # real-time audio scheduling priority
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Office,DEV=0"
      - "--hostID"
      - "office"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_global:
    container_name: snapclient_global
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID         # drop to audio group
      - SETUID         # drop to audio user
      - SYS_NICE       # real-time audio scheduling priority
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Global,DEV=0"
      - "--hostID"
      - "global"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_kitchen:
    container_name: snapclient_kitchen
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as snapclient_office
    cap_add:
      - SETGID
      - SETUID
      - SYS_NICE
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Kitchen,DEV=0"
      - "--hostID"
      - "kitchen"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_main_bedroom:
    container_name: snapclient_main_bedroom
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as snapclient_office
    cap_add:
      - SETGID
      - SETUID
      - SYS_NICE
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Main_Bedroom,DEV=0"
      - "--hostID"
      - "main_bedroom"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_main_bathroom:
    container_name: snapclient_main_bathroom
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as snapclient_office
    cap_add:
      - SETGID
      - SETUID
      - SYS_NICE
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Main_Bathroom,DEV=0"
      - "--hostID"
      - "main_bathroom"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_guest_bathroom:
    container_name: snapclient_guest_bathroom
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as snapclient_office
    cap_add:
      - SETGID
      - SETUID
      - SYS_NICE
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Guest_Bathroom,DEV=0"
      - "--hostID"
      - "guest_bathroom"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_guest_bedroom:
    container_name: snapclient_guest_bedroom
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as snapclient_office
    cap_add:
      - SETGID
      - SETUID
      - SYS_NICE
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Guest_Bedroom,DEV=0"
      - "--hostID"
      - "guest_bedroom"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  # Private registry container removed - using GitLab's built-in registry instead
  # Images now pulled from: gitlab-registry.lab.nkontur.com:443/root/homelab/<image>
  mosquitto:
    container_name: mosquitto
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Entrypoint chowns data/config and drops to mosquitto user
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETGID
      - SETUID
    read_only: true
    tmpfs:
      - /tmp
    image: eclipse-mosquitto:2.1.2-alpine@sha256:9cfdd46ad59f3e3e5f592f6baf57ab23e1ad00605509d0f5c1e9b179c5314d87
    restart: unless-stopped
    networks:
      iot:
        ipv4_address: {{ mosquitto_address }}
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - "{{ docker_persistent_data_path }}/mqtt/conf:/mosquitto/config"
      - mosquitto:/mosquitto/data
      - "{{ docker_persistent_data_path }}/certs:/mosquitto/certs"
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 1883 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  amcrest2mqtt:
    container_name: amcrest2mqtt
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/amcrest2mqtt:latest"
    depends_on:
      mosquitto:
        condition: service_healthy
    restart: unless-stopped
    networks:
      iot:
    deploy:
      resources:
        limits:
          memory: 256M
    environment:
      AMCREST_HOST: 10.6.128.9
      AMCREST_PASSWORD: {{ vault_doorbell_pass | default(lookup('env', 'DOORBELL_PASS')) | trim }}
      MQTT_HOST: "mqtt.lab.nkontur.com"
      MQTT_USERNAME: mosquitto
      MQTT_PASSWORD: {{ vault_mqtt_pass | default(lookup('env', 'MQTT_PASS')) | trim }}
      HOME_ASSISTANT: "true"
      MQTT_TLS_ENABLED: "true"
      MQTT_TLS_CA_CERT: "/etc/ssl/certs/ca-certificates.crt"
      STORAGE_POLL_INTERVAL: 0
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f amcrest2mqtt || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  ambientweather:
    image: ghcr.io/neilenns/ambientweather2mqtt:latest@sha256:a9d9899ebf9c2af9378c275e83ead6ced841fe9d051a6aec4e10e25eb5193e21
    restart: unless-stopped
    container_name: ambientweather
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    networks:
      iot:
        ipv4_address: {{ weather_mqtt_bridge_ip }}
    deploy:
      resources:
        limits:
          memory: 256M
    environment:
      STATION_MAC_ADDRESS: {{ weather_station_mac }}
      MQTT_SERVER: "mqtts://mqtt.lab.nkontur.com:1883"
      TZ: America/New_York
      PORT: 8080
      MQTT_USERNAME: mosquitto
      MQTT_PASSWORD: {{ vault_mqtt_pass | default(lookup('env', 'MQTT_PASS')) | trim }}
      MQTT_REJECT_UNAUTHORIZED: "false"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  zigbee2mqtt:
    container_name: zigbee2mqtt
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    restart: unless-stopped
    image: koenkk/zigbee2mqtt:2@sha256:89cf02f379aa743a68494388e3a26fba7b8c9101f8b452038cc07aeff3fc983c
    depends_on:
      mosquitto:
        condition: service_healthy
    networks:
      iot:
    ports:
      - 8081:8080
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - "{{ docker_persistent_data_path }}/zigbee2mqtt:/app/data"
    environment:
      - TZ=America/New_York
    healthcheck:
      test: ["CMD", "wget", "--spider", "--quiet", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  paperless-ngx:
    image: lscr.io/linuxserver/paperless-ngx:latest@sha256:94ebc3ba985e9548a8a74ebff1d5d51011e78d41ded5fbffee13dceea5b27401
    container_name: paperless-ngx
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID
      - SETGID
      - CHOWN
      - DAC_OVERRIDE
    networks:
      internal:
    deploy:
      resources:
        limits:
          memory: 768M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
      - PAPERLESS_URL=https://paperless-ngx.lab.nkontur.com
    volumes:
      - "{{ docker_persistent_data_path }}/paperless:/config"
      - "/mpool/nextcloud/paperless:/data"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  mopidy:
    image: wernight/mopidy:latest@sha256:e3156f3da69d3e88b191c852d1fa60907c91e36668bca1f5afb5b57b74a8407d
    container_name: mopidy
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    networks:
      iot:
        ipv4_address: {{ mopidy_address }}
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - TZ=America/New_York
    volumes:
      - "{{ docker_persistent_data_path }}/mopidy/media:/var/lib/mopidy/media:ro"
      - "{{ docker_persistent_data_path }}/mopidy/local:/var/lib/mopidy/local"
      - "{{ docker_persistent_data_path }}/mopidy/playlists:/var/lib/mopidy/playlists"
      - "{{ docker_persistent_data_path }}/mopidy/config/mopidy.conf:/config/mopidy.conf"
      - "fifos:/tmp/fifo"
    devices:
      - "/dev/snd"
    ports:
      - "443:6680"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6680"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  nextcloud_db:
    image: mariadb:12.1.2-ubi10@sha256:5ad51257d1c586397a7c86449d1cae9c684989e7efda435c377f8457c296e1e9
    container_name: nextcloud_database
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # MariaDB entrypoint chowns data dir and drops to mysql user
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETGID
      - SETUID
    command: --transaction-isolation=READ-COMMITTED --binlog-format=ROW
    restart: unless-stopped
    networks:
      - nextcloud_internal
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 2G
    volumes:
      - nextcloud_db:/var/lib/mysql
    environment:
      - MYSQL_ROOT_PASSWORD={{ vault_nextcloud_db_password | default(lookup('env', 'NEXTCLOUD_DB_PASSWORD')) | trim }}
      - MYSQL_PASSWORD={{ vault_nextcloud_db_password | default(lookup('env', 'NEXTCLOUD_DB_PASSWORD')) | trim }}
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  nextcloud:
    image: nextcloud:32.0.5-fpm@sha256:859376abea2c2fbee4cd2046cb1d9ed9714d37f200fa8b409bbca10ca3bc52e6
    container_name: nextcloud
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    networks:
      - external
      - nextcloud_internal
    hostname: nkontur.com
    ports:
      - "443:80"
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 2G
    volumes:
      - nextcloud:/data
#      - {{ docker_persistent_data_path }}/nextcloud/config:/var/www/html/config
#      - {{ docker_persistent_data_path }}/nextcloud/mpm_prefork.conf:/etc/apache2/mods-available/mpm_prefork.conf
#      - {{ docker_persistent_data_path }}/nextcloud/config/php.ini:/usr/local/etc/php/php.ini
      - /etc/localtime:/etc/localtime:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/status.php"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  prowlarr:
    image: lscr.io/linuxserver/prowlarr:latest@sha256:b1372ea2bb237e6531412c9cc0436ff31d688be6af4f24aa56269c2d7b0fdc89
    container_name: prowlarr
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    restart: unless-stopped
    ports:
      - "443:7878"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/prowlarr:/config
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9696/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  moltbot-gateway:
    image: gitlab-registry.lab.nkontur.com:443/root/homelab/moltbot:{{ moltbot_image_tag | default('latest') }}
    container_name: moltbot-gateway
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    restart: unless-stopped
    networks:
      internal:
      iot:
      mgmt:
        ipv4_address: {{ moltbot_gateway_ip }}
      moltbot-nginx-bridge:
    extra_hosts:
      - "nkontur.com:172.30.0.10"
      - "www.nkontur.com:172.30.0.10"
    ports:
      - "18789:18789"
    deploy:
      resources:
        limits:
          memory: 3G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18789/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    environment:
      - TZ=America/New_York
      # Core moltbot configuration
      - OPENAI_API_KEY={{ vault_openai_api_key | default(lookup('env', 'OPENAI_API_KEY')) | trim }}
      - CLAWDBOT_GATEWAY_TOKEN={{ vault_moltbot_gateway_token | default(lookup('env', 'MOLTBOT_GATEWAY_TOKEN')) | trim }}
      - TELEGRAM_BOT_TOKEN={{ vault_moltbot_telegram_token | default(lookup('env', 'MOLTBOT_TELEGRAM_TOKEN')) | trim }}
      - GITLAB_TOKEN={{ vault_moltbot_gitlab_token | default(lookup('env', 'MOLTBOT_GITLAB_TOKEN')) | trim }}
      # IPMI credentials removed — use JIT T2 (ipmi resource) instead
      # Web search
      - BRAVE_API_KEY={{ brave_api_key }}
      # Agent platforms
      - ACLAWDEMY_API_KEY={{ vault_aclawdemy_api_key | default(lookup('env', 'ACLAWDEMY_API_KEY')) | trim }}
      # Vault AppRole credentials (for moltbot runtime Vault access & JIT)
      - VAULT_ADDR=https://vault.lab.nkontur.com:8200
      - VAULT_APPROLE_ROLE_ID={{ lookup('env', 'VAULT_APPROLE_ROLE_ID') }}
      - VAULT_APPROLE_SECRET_ID={{ lookup('env', 'VAULT_APPROLE_SECRET_ID') }}
    volumes:
      - {{ docker_persistent_data_path }}/moltbot/data:/home/node/.openclaw
      - {{ docker_persistent_data_path }}/moltbot/mcporter:/home/node/.mcporter
      - {{ docker_persistent_data_path }}/moltbot/workspace:/home/node/.openclaw/workspace
    command: ["node", "dist/index.js", "gateway", "--bind", "lan", "--port", "18789"]
