volumes:
  nextcloud_db:
  wordpress_db:
  ombi:
  fifos:
  # registry volume removed - using GitLab registry instead
  mosquitto:
  vault_data:
  nextcloud:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /mpool/nextcloud/nextcloud
  influxdb-storage:
  grafana-storage:
  otp_data:
  nextcloud_aio_mastercontainer:
    name: nextcloud_aio_mastercontainer
networks:
  mgmt:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.4.0.0/16
            ip_range: 10.4.0.0/18
            aux_addresses:
              shim4: 10.4.0.2
    driver_opts:
      parent: bond0.4
  iot:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.6.0.0/16
            ip_range: 10.6.0.0/18
            aux_addresses:
              shim6: 10.6.0.2
    driver_opts:
      parent: bond0.6
  external:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.2.0.0/16
            ip_range: 10.2.0.0/18
            aux_addresses:
              shim3: 10.2.0.2
    driver_opts:
      parent: bond0.2
  internal:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.3.0.0/16
            ip_range: 10.3.0.0/18
            aux_addresses:
              shim2: 10.3.0.2
    driver_opts:
      parent: bond0.3
  guest:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.5.0.0/16
            ip_range: 10.5.0.0/18
            aux_addresses:
              shim2: 10.5.0.2
    driver_opts:
      parent: bond0.5
  wordpress_internal:
    driver: bridge
  nextcloud_internal:
    driver: bridge
services:
  # Docker socket proxy - filtered access for CI builds
  # Only allows build/image operations, blocks exec/start/stop
  docker-socket-proxy:
    container_name: docker-socket-proxy
    image: tecnativa/docker-socket-proxy:latest
    restart: unless-stopped
    networks:
      mgmt:
        ipv4_address: {{ docker_socket_proxy_ip }}
    deploy:
      resources:
        limits:
          memory: 128M
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      # Allow build operations
      - BUILD=1
      - IMAGES=1
      - COMMIT=1
      # Allow auth for registry push
      - AUTH=1
      # Deny dangerous operations
      - CONTAINERS=0
      - EXEC=0
      - NETWORKS=0
      - VOLUMES=0
      - POST=1  # Needed for build/push
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:2375/_ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  plex:
    container_name: plex
    ports:
      - "32400/tcp"
    image: plexinc/pms-docker
    networks:
      - external
      - internal
    restart: unless-stopped
    # GPU disabled until NVML driver/library mismatch is fixed
    # runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 2G
    environment:
      - TZ=America/New_York
      # - NVIDIA_VISIBLE_DEVICES=all
      # - NVIDIA_DRIVER_CAPABILITIES=compute,video,utility
    volumes:
      - /mpool/plex/config:/config
      - /mpool/plex/transcode:/transcode
      - /mpool/plex:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:32400/identity"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
  pihole:
    container_name: pihole
    image: pihole/pihole:2025.11.1
    ports:
      - "443:80"
    networks:
      internal:
        ipv4_address: {{ pihole_ip }}
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      TZ: America/New_York
      WEBPASSWORD: {{ vault_pihole_password | default(lookup('env', 'PIHOLE_PASSWORD')) | trim }}
    volumes:
      - {{ docker_persistent_data_path }}/pihole/conf:/etc/pihole
      - {{ docker_persistent_data_path }}/pihole/dnsmasq:/etc/dnsmasq.d
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "dig", "+short", "+norecurse", "+retry=0", "@127.0.0.1", "pi.hole"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  iperf3:
    container_name: iperf3
    image: networkstatic/iperf3
    ports:
      - "5201/tcp"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 128M
    command:
      - "-s"
    restart: unless-stopped
  nginx: 
    image: nginx:1.27-alpine
    container_name: nginx
    networks:
      external:
        ipv4_address: {{ nginx_ip }}
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pgrep nginx > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    volumes:
      - {{ docker_persistent_data_path }}/nginx/conf:/etc/nginx
      - {{ docker_persistent_data_path }}/nginx/webroot:/data/webroot
      - /var/log/nginx:/data/log
      - {{ docker_persistent_data_path }}/certs:/data/certs
  lab_nginx: 
    image: nginx:1.27-alpine
    container_name: lab_nginx
    networks:
      internal:
        ipv4_address: {{ lab_nginx_ip }}
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pgrep nginx > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    volumes:
      - {{ docker_persistent_data_path }}/lab_nginx/conf:/etc/nginx
      - /var/log/lab_nginx:/data/log
      - {{ docker_persistent_data_path }}/certs:/data/certs
  iot_nginx: 
    image: nginx:1.27-alpine
    container_name: iot_nginx
    networks:
      iot:
        ipv4_address: {{ iot_nginx_ip }}
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pgrep nginx > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    volumes:
      - {{ docker_persistent_data_path }}/iot_nginx/conf:/etc/nginx
      - /var/log/iot_nginx:/data/log
      - {{ docker_persistent_data_path }}/certs:/data/certs
  ombi:
    image: linuxserver/ombi
    container_name: ombi
    networks:
      - external
      - internal
    deploy:
      resources:
        limits:
          memory: 768M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
      - BASE_URL=/plex-requests
    volumes:
      - ombi:/config
      - /etc/ssl/certs:/etc/ssl/certs
      - /etc/ssl/private:/etc/ssl/private
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3579/api/v1/Status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  nzbget:
    image: linuxserver/nzbget
    container_name: nzbget
    ports:
      - "6789:6789"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/nzbget:/config
      - /mpool/plex/Movies:/movies
      - /mpool/samba_share/nfs:/remote
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6789"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  radarr:
    image: linuxserver/radarr:latest
    container_name: radarr
    ports:
      - "443:7878"
    networks:
      - internal
    # Memory increased from 1G to 2G: library has 5300+ movies (28MB API payload).
    # 1G limit causes memory pressure during large serialization + concurrent tasks.
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/radarr:/config
      - /mpool/plex/Movies:/movies
      - /mpool/samba_share/nfs:/remote
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7878/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  jackett:
    image: linuxserver/jackett
    container_name: jackett
    ports:
      - "443:9117"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/jackett:/config
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9117/UI/Login"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  sonarr:
    image: linuxserver/sonarr
    container_name: sonarr
    ports:
      - "443:8989"
    networks:
    - internal
    # 308K history records — Sonarr benefits from memory headroom for DB operations.
    # Added reservation to prevent OOM kills during heavy queries.
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/sonarr:/config
      - /mpool/plex/TV:/tv
      - /mpool/samba_share/nfs:/remote
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8989/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  booksonic:
    image: izderadicka/audioserve
    container_name: audioserve
    ports:
      - "443:3000"
    networks:
      - external
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
      - AUDIOSERVE_SHARED_SECRET={{ vault_audioserve_secret | default(lookup('env', 'AUDIOSERVE_SECRET')) | trim }}
      - VIRTUAL_HOST=audioserve.nkontur.com
      - LETSENCRYPT_HOST=audioserve.nkontur.com
    command: /audiobooks
    volumes:
      - /mpool/audioserve/audiobooks:/audiobooks
      - /mpool/audioserve/data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  influxdb:
    container_name: influxdb
    ports:
      - "443:8086"
    networks:
      internal:
        ipv4_address: {{ influxdb_ip }}
    image: influxdb:latest
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 4G
    volumes:
        - influxdb-storage:/var/lib/influxdb2
        - {{ docker_persistent_data_path }}/influxdb/config:/etc/influxdb2
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD={{ vault_influxdb_password | default(lookup('env', 'INFLUXDB_PASSWORD')) | trim }}
      - DOCKER_INFLUXDB_INIT_ORG=homelab
      - DOCKER_INFLUXDB_INIT_BUCKET=metrics
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN={{ vault_influxdb_admin_token | default(lookup('env', 'INFLUXDB_ADMIN_TOKEN')) | trim }}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8086/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  grafana:
    container_name: grafana
    image: grafana/grafana:latest
    ports:
      - "443:3000"
    networks:
      - internal
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G
    volumes:
      - grafana-storage:/var/lib/grafana
      - {{ docker_persistent_data_path }}/grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD={{ vault_grafana_admin_password | default(lookup('env', 'GRAFANA_ADMIN_PASSWORD')) | trim }}
      - GF_SERVER_ROOT_URL=https://grafana.lab.nkontur.com
      - GF_USERS_ALLOW_SIGN_UP=false
      - INFLUXDB_TOKEN={{ vault_influxdb_admin_token | default(lookup('env', 'INFLUXDB_ADMIN_TOKEN')) | trim }}
      # SMTP for email alerts
      - GF_SMTP_ENABLED=true
      - GF_SMTP_HOST=smtp.gmail.com:587
      - GF_SMTP_USER=konoahko@gmail.com
      - GF_SMTP_PASSWORD={{ vault_grafana_smtp_password | default(lookup('env', 'GRAFANA_SMTP_PASSWORD')) | trim }}
      - GF_SMTP_FROM_ADDRESS=konoahko@gmail.com
      - GF_SMTP_FROM_NAME=Grafana Homelab
      - GF_SMTP_STARTTLS_POLICY=MandatoryStartTLS
    depends_on:
      - influxdb
      - loki
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  loki:
    container_name: loki
    image: grafana/loki:3.5.0
    ports:
      - "3100:3100"
    networks:
      internal:
        ipv4_address: {{ loki_ip }}
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
    volumes:
      - {{ docker_persistent_data_path }}/loki/data:/loki
      - {{ docker_persistent_data_path }}/loki/local-config.yaml:/etc/loki/local-config.yaml:ro
    command: -config.file=/etc/loki/local-config.yaml
    # Loki uses default json-file logging to avoid circular dependency
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  promtail:
    image: grafana/promtail:3.5.0@sha256:450959b22a0eabbf7c364dc3b0c6483573a22012ab853fa5e88025fdd12deda5
    container_name: promtail
    restart: unless-stopped
    depends_on:
      - loki
    volumes:
      - /var/log:/var/log:ro
      - /run/log/journal:/run/log/journal:ro
      - /etc/machine-id:/etc/machine-id:ro
      - {{ docker_persistent_data_path }}/promtail:/tmp/positions
      - {{ docker_persistent_data_path }}/promtail/config.yml:/etc/promtail/config.yml:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      internal:
        ipv4_address: {{ promtail_ip }}
    mem_limit: 128m
    cpus: 0.25
    # Use json-file logging to avoid circular dependency (same as Loki)
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9080/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
  gitlab:
    container_name: gitlab
    ports:
      - "22/tcp"
      - "443:80"
    networks:
      - internal
    image: 'gitlab/gitlab-ee:latest'
    restart: unless-stopped
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      nproc: 65536
    deploy:
      resources:
        limits:
          memory: 14G
        reservations:
          memory: 4G
    volumes:
      - '{{ docker_persistent_data_path }}/gitlab/config:/etc/gitlab'
      - '{{ docker_persistent_data_path }}/gitlab/logs:/var/log/gitlab'
      - '{{ docker_persistent_data_path }}/gitlab/data:/var/opt/gitlab'
      - '{{ docker_persistent_data_path }}/gitlab/license_encryption_key.pub:/opt/gitlab/embedded/service/gitlab-rails/.license_encryption_key.pub:ro'
    environment:
      MALLOC_CONF: 'dirty_decay_ms:1000,muzzy_decay_ms:1000'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/-/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 300s

  # HashiCorp Vault - Secrets Management
  # Initial setup uses file storage; will be configured for GitLab JWT auth in future MR
  vault:
    container_name: vault
    image: hashicorp/vault:1.21
    restart: unless-stopped
    networks:
      internal:
        ipv4_address: {{ vault_ip }}
    ports:
      - "8200:8200"
    deploy:
      resources:
        limits:
          memory: 768M
    cap_add:
      - IPC_LOCK  # Required for Vault to lock memory and prevent secrets from being swapped
    volumes:
      - vault_data:/vault/file
      - '{{ docker_persistent_data_path }}/vault/config:/vault/config:ro'
      - '{{ docker_persistent_data_path }}/certs:/vault/certs:ro'
      - '{{ docker_persistent_data_path }}/vault/scripts/auto-unseal.sh:/vault/scripts/auto-unseal.sh:ro'
      - '{{ docker_persistent_data_path }}/vault/unseal:/vault/unseal:ro'
    environment:
      - VAULT_ADDR=https://127.0.0.1:8200
      - VAULT_API_ADDR=https://{{ vault_ip }}:8200
      - VAULT_CACERT=/vault/certs/nkontur.com/live/iot.lab.nkontur.com-0003/chain.pem
      # Skip TLS verify for healthcheck using loopback
      - VAULT_SKIP_VERIFY=true
      # Path to unseal keys file inside the container
      - VAULT_UNSEAL_KEYS_FILE=/vault/unseal/unseal-keys
    entrypoint: /bin/sh
    command: ["/vault/scripts/auto-unseal.sh"]
    healthcheck:
      test: ["CMD", "sh", "-c", "vault status -address=https://127.0.0.1:8200; [ $? -le 2 ]"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  deluge:
    container_name: deluge
    ports:
      - "443:8112"
    image: 'lscr.io/linuxserver/deluge:latest'
    restart: unless-stopped
    networks:
      internal:
        ipv4_address: {{ deluge_ip }}
    # Memory limit set to 2G after reducing libtorrent disk cache.
    # Default cache_size (512 pieces) causes 28GB+ RAM usage with large torrents.
    # Reduced to 128 pieces via core.conf — see configure-docker ansible role.
    deploy:
      resources:
        limits:
          memory: 2G
    volumes:
      - '{{ docker_persistent_data_path }}/deluge:/config'
      - '/mpool/samba_share/nfs:/downloads'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8112"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  bitwarden:
    container_name: bitwarden
    ports:
      - "443:80"
    image: 'vaultwarden/server:latest'
    restart: unless-stopped
    networks:
      - external
    deploy:
      resources:
        limits:
          memory: 512M
    volumes:
      - '{{ docker_persistent_data_path }}/bitwarden/data/:/data/'
    env_file:
      - '{{ docker_persistent_data_path }}/bitwarden/global.override.env'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/alive"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  blog:
    image: wordpress
    container_name: wordpress
    ports:
      - "443:80"
    networks:
      - external
      - wordpress_internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      WORDPRESS_DB_HOST: wordpress_db
      WORDPRESS_DB_USER: wordpress
      WORDPRESS_DB_PASSWORD: {{ vault_wordpress_db_password | default(lookup('env', 'WORDPRESS_DB_PASSWORD')) | trim }}
      WORDPRESS_DB_NAME: wordpress
    volumes:
      - {{ docker_persistent_data_path }}/wordpress/html:/var/www/html
      - {{ docker_persistent_data_path }}/wordpress/config:/etc/wordpress
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  wordpress_db:
    image: mysql
    container_name: wordpress_db
    networks:
      - wordpress_internal
    restart: unless-stopped
    cap_add:
      - SYS_NICE  # CAP_SYS_NICE
    deploy:
      resources:
        limits:
          memory: 768M
    environment:
      MYSQL_DATABASE: wordpress
      MYSQL_USER: wordpress
      MYSQL_PASSWORD: {{ vault_wordpress_db_password | default(lookup('env', 'WORDPRESS_DB_PASSWORD')) | trim }}
      MYSQL_ROOT_PASSWORD: {{ vault_wordpress_db_password | default(lookup('env', 'WORDPRESS_DB_PASSWORD')) | trim }}
    volumes:
      - wordpress_db:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  diagram:
    container_name: diagram
    ports:
      - "443:8080"
    networks:
      internal:
    image: jgraph/drawio
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 768M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  homeassistant:
    container_name: homeassistant
    networks:
      - internal
      - iot
      - external
    depends_on:
      - plex
      - amcrest2mqtt
      - snapserver
    ports:
      - "443:8123"
    devices: 
      - "/dev/serial/by-id/usb-Prolific_Technology_Inc._USB-Serial_Controller_ETDRb11A920-if00-port0:/dev/ttyUSB0"
      - "/dev/serial/by-id/usb-Prolific_Technology_Inc._USB-Serial_Controller-if00-port0:/dev/ttyUSB1"
    image: "homeassistant/home-assistant:2025.12"
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    volumes:
      - "{{ docker_persistent_data_path }}/homeassistant:/config"
      - /etc/localtime:/etc/localtime:ro
    command: ["/bin/bash", "-c", "ip route del default; ip route add default via 10.3.0.1 dev eth1; /init"]
    cap_add:
      - NET_ADMIN # So we can override default route
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8123/api/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
  snapserver:
    container_name: snapserver
    hostname: snapserver
    networks:
      iot:
        ipv4_address: {{ snapserver_address }}
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapcast:latest"
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - "{{ docker_persistent_data_path }}/snapserver/snapserver.conf:/etc/snapserver.conf"
      - "{{ docker_persistent_data_path }}/snapserver/server.json:/root/.config/snapserver/server.json"
      - "/tmp/snapfifo:/tmp/snapfifo"
      - "fifos:/tmp/fifo"
    restart: unless-stopped
    devices:
      - "/dev/snd"
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 1704 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  snapclient_office:
    container_name: snapclient_office
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Office,DEV=0"
      - "--hostID"
      - "office"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_global:
    container_name: snapclient_global
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Global,DEV=0"
      - "--hostID"
      - "global"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_kitchen:
    container_name: snapclient_kitchen
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Kitchen,DEV=0"
      - "--hostID"
      - "kitchen"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_main_bedroom:
    container_name: snapclient_main_bedroom
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Main_Bedroom,DEV=0"
      - "--hostID"
      - "main_bedroom"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_main_bathroom:
    container_name: snapclient_main_bathroom
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Main_Bathroom,DEV=0"
      - "--hostID"
      - "main_bathroom"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_guest_bathroom:
    container_name: snapclient_guest_bathroom
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Guest_Bathroom,DEV=0"
      - "--hostID"
      - "guest_bathroom"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_guest_bedroom:
    container_name: snapclient_guest_bedroom
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Guest_Bedroom,DEV=0"
      - "--hostID"
      - "guest_bedroom"
    healthcheck:
      test: ["CMD-SHELL", "pgrep snapclient || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  # Private registry container removed - using GitLab's built-in registry instead
  # Images now pulled from: gitlab-registry.lab.nkontur.com:443/root/homelab/<image>
  mosquitto:
    container_name: mosquitto
    image: eclipse-mosquitto
    restart: unless-stopped
    networks:
      iot:
        ipv4_address: {{ mosquitto_address }}
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - "{{ docker_persistent_data_path }}/mqtt/conf:/mosquitto/config"
      - mosquitto:/mosquitto/data
      - "{{ docker_persistent_data_path }}/certs:/mosquitto/certs"
    healthcheck:
      test: ["CMD", "mosquitto_sub", "-t", "$$SYS/#", "-C", "1", "-W", "3"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  amcrest2mqtt:
    container_name: amcrest2mqtt
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/amcrest2mqtt:latest"
    depends_on:
      - mosquitto
    restart: unless-stopped
    networks:
      iot:
    deploy:
      resources:
        limits:
          memory: 256M
    environment:
      AMCREST_HOST: 10.6.128.9
      AMCREST_PASSWORD: {{ vault_doorbell_pass | default(lookup('env', 'DOORBELL_PASS')) | trim }}
      MQTT_HOST: "mqtt.lab.nkontur.com"
      MQTT_USERNAME: mosquitto
      MQTT_PASSWORD: {{ vault_mqtt_pass | default(lookup('env', 'MQTT_PASS')) | trim }}
      HOME_ASSISTANT: "true"
      MQTT_TLS_ENABLED: "true"
      MQTT_TLS_CA_CERT: "/etc/ssl/certs/ca-certificates.crt"
      STORAGE_POLL_INTERVAL: 0
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f amcrest2mqtt || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  ambientweather:
    image: ghcr.io/neilenns/ambientweather2mqtt:latest
    restart: unless-stopped
    container_name: ambientweather
    networks:
      iot:
        ipv4_address: {{ weather_mqtt_bridge_ip }}
    deploy:
      resources:
        limits:
          memory: 256M
    environment:
      STATION_MAC_ADDRESS: {{ weather_station_mac }}
      MQTT_SERVER: "mqtts://mqtt.lab.nkontur.com:1883"
      TZ: America/New_York
      PORT: 8080
      MQTT_USERNAME: mosquitto
      MQTT_PASSWORD: {{ vault_mqtt_pass | default(lookup('env', 'MQTT_PASS')) | trim }}
      MQTT_REJECT_UNAUTHORIZED: "false"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  zigbee2mqtt:
    container_name: zigbee2mqtt
    restart: unless-stopped
    image: koenkk/zigbee2mqtt
    depends_on:
      - mosquitto
    networks:
      iot:
    ports:
      - 8081:8080
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - "{{ docker_persistent_data_path }}/zigbee2mqtt:/app/data"
    environment:
      - TZ=America/New_York
    healthcheck:
      test: ["CMD", "wget", "--spider", "--quiet", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  paperless-ngx:
    image: lscr.io/linuxserver/paperless-ngx:latest
    container_name: paperless-ngx
    networks:
      internal:
    deploy:
      resources:
        limits:
          memory: 768M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
      - PAPERLESS_URL=https://paperless-ngx.lab.nkontur.com
    volumes:
      - "{{ docker_persistent_data_path }}/paperless:/config"
      - "/mpool/nextcloud/paperless:/data"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  mopidy:
    image: wernight/mopidy
    container_name: mopidy
    networks:
      iot:
        ipv4_address: {{ mopidy_address }}
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - TZ=America/New_York
    volumes:
      - "{{ docker_persistent_data_path }}/mopidy/media:/var/lib/mopidy/media:ro"
      - "{{ docker_persistent_data_path }}/mopidy/local:/var/lib/mopidy/local"
      - "{{ docker_persistent_data_path }}/mopidy/playlists:/var/lib/mopidy/playlists"
      - "{{ docker_persistent_data_path }}/mopidy/config/mopidy.conf:/config/mopidy.conf"
      - "fifos:/tmp/fifo"
    devices:
      - "/dev/snd"
    ports:
      - "443:6680"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6680"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  nextcloud_db:
    image: mariadb
    container_name: nextcloud_database
    command: --transaction-isolation=READ-COMMITTED --binlog-format=ROW
    restart: unless-stopped
    networks:
      - nextcloud_internal
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 2G
    volumes:
      - nextcloud_db:/var/lib/mysql
    environment:
      - MYSQL_ROOT_PASSWORD={{ vault_nextcloud_db_password | default(lookup('env', 'NEXTCLOUD_DB_PASSWORD')) | trim }}
      - MYSQL_PASSWORD={{ vault_nextcloud_db_password | default(lookup('env', 'NEXTCLOUD_DB_PASSWORD')) | trim }}
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  nextcloud:
    image: nextcloud:32.0.1
    container_name: nextcloud
    networks:
      - external
      - nextcloud_internal
    hostname: nkontur.com
    ports:
      - "443:80"
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 2G
    volumes:
      - nextcloud:/data
#      - {{ docker_persistent_data_path }}/nextcloud/config:/var/www/html/config
#      - {{ docker_persistent_data_path }}/nextcloud/mpm_prefork.conf:/etc/apache2/mods-available/mpm_prefork.conf
#      - {{ docker_persistent_data_path }}/nextcloud/config/php.ini:/usr/local/etc/php/php.ini
      - /etc/localtime:/etc/localtime:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/status.php"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  prowlarr:
    image: lscr.io/linuxserver/prowlarr:latest
    container_name: prowlarr
    restart: unless-stopped
    ports:
      - "443:7878"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/prowlarr:/config
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9696/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  moltbot-gateway:
    image: gitlab-registry.lab.nkontur.com:443/root/homelab/moltbot:{{ moltbot_image_tag | default('latest') }}
    container_name: moltbot-gateway
    restart: unless-stopped
    networks:
      internal:
      iot:
      mgmt:
        ipv4_address: {{ moltbot_gateway_ip }}
    ports:
      - "18789:18789"
    deploy:
      resources:
        limits:
          memory: 3G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18789/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    environment:
      - TZ=America/New_York
      # Core moltbot configuration
      - OPENAI_API_KEY={{ vault_openai_api_key | default(lookup('env', 'OPENAI_API_KEY')) | trim }}
      - CLAWDBOT_GATEWAY_TOKEN={{ vault_moltbot_gateway_token | default(lookup('env', 'MOLTBOT_GATEWAY_TOKEN')) | trim }}
      - TELEGRAM_BOT_TOKEN={{ vault_moltbot_telegram_token | default(lookup('env', 'MOLTBOT_TELEGRAM_TOKEN')) | trim }}
      - HASS_URL=http://homeassistant:8123
      - HASS_TOKEN={{ vault_hass_token | default(lookup('env', 'HASS_TOKEN')) | trim }}
      - GITLAB_TOKEN={{ vault_moltbot_gitlab_token | default(lookup('env', 'MOLTBOT_GITLAB_TOKEN')) | trim }}
      # Media management APIs (internal network)
      - RADARR_URL=http://radarr:7878
      - RADARR_API_KEY={{ vault_radarr_api_key | default(lookup('env', 'RADARR_API_KEY')) | trim }}
      - SONARR_URL=http://sonarr:8989
      - SONARR_API_KEY={{ vault_sonarr_api_key | default(lookup('env', 'SONARR_API_KEY')) | trim }}
      - PROWLARR_URL=http://prowlarr:9696
      - PROWLARR_API_KEY={{ vault_prowlarr_api_key | default(lookup('env', 'PROWLARR_API_KEY')) | trim }}
      - PLEX_URL=http://plex:32400
      - PLEX_TOKEN={{ vault_plex_token | default(lookup('env', 'PLEX_TOKEN')) | trim }}
      - OMBI_URL=http://ombi:3579
      - OMBI_API_KEY={{ vault_ombi_api_key | default(lookup('env', 'OMBI_API_KEY')) | trim }}
      # Download clients
      - NZBGET_URL=http://nzbget:6789
      - NZBGET_USERNAME={{ vault_nzbget_username | default(lookup('env', 'NZBGET_USERNAME')) | trim }}
      - NZBGET_PASSWORD={{ vault_nzbget_password | default(lookup('env', 'NZBGET_PASSWORD')) | trim }}
      - DELUGE_URL=http://deluge:8112
      - DELUGE_PASSWORD={{ vault_deluge_password | default(lookup('env', 'DELUGE_PASSWORD')) | trim }}
      # Document management
      - PAPERLESS_URL=http://paperless-ngx:8000
      - PAPERLESS_TOKEN={{ vault_paperless_token | default(lookup('env', 'PAPERLESS_TOKEN')) | trim }}
      # Metrics/monitoring
      - INFLUXDB_URL=http://influxdb:8086
      - INFLUXDB_TOKEN={{ vault_influxdb_token | default(lookup('env', 'INFLUXDB_TOKEN')) | trim }}
      # Grafana API access
      - GRAFANA_URL={{ grafana_url }}
      - GRAFANA_TOKEN={{ grafana_token }}
      # Tailscale API access
      - TAILSCALE_API_TOKEN={{ vault_tailscale_api_token | default(lookup('env', 'TAILSCALE_API_TOKEN')) | trim }}
      # IPMI access for server management
      - IPMI_USER={{ vault_ipmi_user | default(lookup('env', 'IPMI_USER')) | trim }}
      - IPMI_PASSWORD={{ vault_ipmi_password | default(lookup('env', 'IPMI_PASSWORD')) | trim }}
      # Web search
      - BRAVE_API_KEY={{ brave_api_key }}
      # Agent platforms
      - ACLAWDEMY_API_KEY={{ vault_aclawdemy_api_key | default(lookup('env', 'ACLAWDEMY_API_KEY')) | trim }}
      # Email access (for job application verification codes, etc.)
      - GMAIL_EMAIL={{ vault_gmail_email | default(lookup('env', 'GMAIL_EMAIL')) | trim }}
      - GMAIL_APP_PASSWORD={{ vault_gmail_app_password | default(lookup('env', 'GMAIL_APP_PASSWORD')) | trim }}
      # Vault AppRole credentials (for moltbot runtime Vault access)
      - VAULT_ADDR=https://vault.lab.nkontur.com:8200
      - VAULT_APPROLE_ROLE_ID={{ lookup('env', 'VAULT_APPROLE_ROLE_ID') }}
      - VAULT_APPROLE_SECRET_ID={{ lookup('env', 'VAULT_APPROLE_SECRET_ID') }}
    volumes:
      - {{ docker_persistent_data_path }}/moltbot/data:/home/node/.openclaw
      - {{ docker_persistent_data_path }}/moltbot/mcporter:/home/node/.mcporter
      - {{ docker_persistent_data_path }}/moltbot/workspace:/home/node/.openclaw/workspace
    command: ["node", "dist/index.js", "gateway", "--bind", "lan", "--port", "18789"]
