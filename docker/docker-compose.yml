volumes:
  nextcloud_db:
  wordpress_db:
  ombi:
  fifos:
  # registry volume removed - using GitLab registry instead
  mosquitto:
  vault_data:
  nextcloud:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /mpool/nextcloud/nextcloud
  influxdb-storage:
  grafana-storage:
  otp_data:
  nextcloud_aio_mastercontainer:
    name: nextcloud_aio_mastercontainer
networks:
  mgmt:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.4.0.0/16
            ip_range: 10.4.0.0/18
            aux_addresses:
              shim4: 10.4.0.2
    driver_opts:
      parent: bond0.4
  iot:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.6.0.0/16
            ip_range: 10.6.0.0/18
            aux_addresses:
              shim6: 10.6.0.2
    driver_opts:
      parent: bond0.6
  external:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.2.0.0/16
            ip_range: 10.2.0.0/18
            aux_addresses:
              shim3: 10.2.0.2
    driver_opts:
      parent: bond0.2
  internal:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.3.0.0/16
            ip_range: 10.3.0.0/18
            aux_addresses:
              shim2: 10.3.0.2
    driver_opts:
      parent: bond0.3
  guest:
    driver: macvlan
    ipam:
      config:
          - subnet: 10.5.0.0/16
            ip_range: 10.5.0.0/18
            aux_addresses:
              shim2: 10.5.0.2
    driver_opts:
      parent: bond0.5
  wordpress_internal:
    driver: bridge
  nextcloud_internal:
    driver: bridge
  jit-bridge:
    driver: bridge
    internal: true
  openclaw-nginx-bridge:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.30.0.0/24
  proxy-internal:
    driver: bridge
    internal: true
  loki-backend:
    driver: bridge
    internal: true
services:
  # Docker socket proxy - filtered access for CI builds
  # Only allows build/image operations, blocks exec/start/stop
  # TLS termination handled by lab_nginx on the proxy-internal network
  docker-socket-proxy:
    container_name: docker-socket-proxy
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: tecnativa/docker-socket-proxy:0.3.0@sha256:9e4b9e7517a6b660f2cc903a19b257b1852d5b3344794e3ea334ff00ae677ac2
    restart: unless-stopped
    networks:
      proxy-internal:
    deploy:
      resources:
        limits:
          memory: 128M
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      # Allow build operations
      - BUILD=1
      - IMAGES=1
      - COMMIT=1
      # Allow auth for registry push
      - AUTH=1
      # Deny dangerous operations
      - CONTAINERS=0
      - EXEC=0
      - NETWORKS=0
      - VOLUMES=0
      - POST=1  # Needed for build/push
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:2375/_ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  plex:
    container_name: plex
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "32400/tcp"
    image: plexinc/pms-docker:1.43.0.10492-121068a07@sha256:1131c4cd21fa22f8196f749f1dbb69af306776c3c83c7f5b061e51dc49bcff7f
    networks:
      - external
      - internal
    restart: unless-stopped
    # GPU disabled until NVML driver/library mismatch is fixed
    # runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 2G
    environment:
      - TZ=America/New_York
      # - NVIDIA_VISIBLE_DEVICES=all
      # - NVIDIA_DRIVER_CAPABILITIES=compute,video,utility
    volumes:
      - /mpool/plex/config:/config
      - /mpool/plex/transcode:/transcode
      - /mpool/plex:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:32400/identity"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
  pihole:
    container_name: pihole
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN            # pihole user/group setup changes file ownership
      - DAC_OVERRIDE     # access config files across pihole/root users
      - FOWNER           # chmod on gravity.db during gravity updates
      - NET_ADMIN        # DHCP server and network interface management
      - NET_BIND_SERVICE # DNS listener on port 53
      - NET_RAW          # DHCP raw socket access
      - SETGID           # drop to pihole group
      - SETUID           # drop to pihole user
    image: pihole/pihole:2026.02.0@sha256:ee348529cea9601df86ad94d62a39cad26117e1eac9e82d8876aa0ec7fe1ba27
    ports:
      - "443:80"
    networks:
      internal:
        ipv4_address: {{ pihole_ip }}
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      TZ: America/New_York
      WEBPASSWORD: {{ vault_pihole_password | default(lookup('env', 'PIHOLE_PASSWORD')) | trim | replace("$", "$$") }}
      DNSMASQ_USER: root
    volumes:
      - {{ docker_persistent_data_path }}/pihole/conf:/etc/pihole
      - {{ docker_persistent_data_path }}/pihole/dnsmasq:/etc/dnsmasq.d
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "dig", "+short", "+norecurse", "+retry=0", "@127.0.0.1", "pi.hole"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  iperf3:
    container_name: iperf3
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    tmpfs:
      - /tmp
    image: networkstatic/iperf3:latest@sha256:07dcca91c0e47d82d83d91de8c3d46b840eef4180499456b4fa8d6dadb46b6c8
    ports:
      - "5201/tcp"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 128M
    command:
      - "-s"
    restart: unless-stopped
  nginx: 
    image: nginx:1.29.5-alpine3.23-perl@sha256:db0a3121225ffbc32fa36cc7a02c6c11002fec0c7fd2596ba204a811cc4a71dc
    container_name: nginx
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # nginx master runs as root, workers drop to nginx user; binds to port 80
    cap_add:
      - CHOWN            # manage log/pid file ownership
      - DAC_OVERRIDE     # read config and write logs across users
      - NET_BIND_SERVICE # bind to port 80
      - SETGID           # worker processes drop to nginx group
      - SETUID           # worker processes drop to nginx user
    read_only: true
    tmpfs:
      - /tmp
      - /run
      - /var/cache/nginx
    networks:
      external:
        ipv4_address: {{ nginx_ip }}
      jit-bridge:
      openclaw-nginx-bridge:
        ipv4_address: 172.30.0.10
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pgrep nginx > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    volumes:
      - {{ docker_persistent_data_path }}/nginx/conf:/etc/nginx
      - {{ docker_persistent_data_path }}/nginx/webroot:/data/webroot
      - /var/log/nginx:/data/log
      - {{ docker_persistent_data_path }}/certs:/data/certs
  lab_nginx: 
    image: nginx:1.29.5-alpine3.23-perl@sha256:db0a3121225ffbc32fa36cc7a02c6c11002fec0c7fd2596ba204a811cc4a71dc
    container_name: lab_nginx
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as nginx above
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - NET_BIND_SERVICE
      - SETGID
      - SETUID
    read_only: true
    tmpfs:
      - /tmp
      - /run
      - /var/cache/nginx
    networks:
      internal:
        ipv4_address: {{ lab_nginx_ip }}
      proxy-internal:
      loki-backend:
    restart: unless-stopped
    depends_on:
      docker-socket-proxy:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pgrep nginx > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    volumes:
      - {{ docker_persistent_data_path }}/lab_nginx/conf:/etc/nginx
      - /var/log/lab_nginx:/data/log
      - {{ docker_persistent_data_path }}/certs:/data/certs
      - {{ docker_persistent_data_path }}/docker-proxy-tls:/data/docker-proxy-tls:ro
  iot_nginx: 
    image: nginx:1.29.5-alpine3.23-perl@sha256:db0a3121225ffbc32fa36cc7a02c6c11002fec0c7fd2596ba204a811cc4a71dc
    container_name: iot_nginx
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as nginx above
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - NET_BIND_SERVICE
      - SETGID
      - SETUID
    read_only: true
    tmpfs:
      - /tmp
      - /run
      - /var/cache/nginx
    networks:
      iot:
        ipv4_address: {{ iot_nginx_ip }}
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pgrep nginx > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    volumes:
      - {{ docker_persistent_data_path }}/iot_nginx/conf:/etc/nginx
      - /var/log/iot_nginx:/data/log
      - {{ docker_persistent_data_path }}/certs:/data/certs
  ombi:
    image: linuxserver/ombi:4.53.4@sha256:fcad7b900ab2bc910f8271435953d99fd8dd4a1d877d09e478c9183bd4591193
    container_name: ombi
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    networks:
      - external
      - internal
    deploy:
      resources:
        limits:
          memory: 768M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
      - BASE_URL=/plex-requests
    volumes:
      - ombi:/config
      - /etc/ssl/certs:/etc/ssl/certs
      - /etc/ssl/private:/etc/ssl/private
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3579/api/v1/Status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  nzbget:
    image: linuxserver/nzbget:26.0.20260213@sha256:3709b8e5c5a6a23cfc25b9de0877be164156392708dc95250291e6d9e571a342
    container_name: nzbget
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
      - FOWNER         # s6-overlay chmods config files owned by PUID
    ports:
      - "127.0.0.1:6789:6789"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/nzbget:/config
      - /mpool/plex/Movies:/movies
      - /mpool/samba_share/nfs:/remote
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6789"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  radarr:
    image: linuxserver/radarr:6.0.4.10291-ls292@sha256:f08dda38e7d12e5a722d9a5cb6e54acaf63c8598fefeefec88effe0c0d0038dd
    container_name: radarr
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "443:7878"
    networks:
      - internal
    # Memory increased from 1G to 2G: library has 5300+ movies (28MB API payload).
    # 1G limit causes memory pressure during large serialization + concurrent tasks.
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/radarr:/config
      - /mpool/plex/Movies:/movies
      - /mpool/samba_share/nfs:/remote
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7878/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  jackett:
    image: linuxserver/jackett:0.24.1193@sha256:f2e811e6ed87fb4953b5b3c127594ab7ce3bfe05e831631fce0c9ade120c50b5
    container_name: jackett
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "443:9117"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/jackett:/config
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9117/UI/Login"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  sonarr:
    image: linuxserver/sonarr:4.0.16.2944-ls303@sha256:37be832b78548e3f55f69c45b50e3b14d18df1b6def2a4994258217e67efb1a1
    container_name: sonarr
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "443:8989"
    networks:
    - internal
    # 308K history records — Sonarr benefits from memory headroom for DB operations.
    # Added reservation to prevent OOM kills during heavy queries.
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/sonarr:/config
      - /mpool/plex/TV:/tv
      - /mpool/samba_share/nfs:/remote
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8989/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  booksonic:
    image: izderadicka/audioserve:latest@sha256:c3609321701765671cae121fc0f61db122e8c124643c04770fbc9326c74b18e3
    container_name: audioserve
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ports:
      - "443:3000"
    networks:
      - external
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
      - AUDIOSERVE_SHARED_SECRET={{ vault_audioserve_secret | default(lookup('env', 'AUDIOSERVE_SECRET')) | trim | replace("$", "$$") }}
      - VIRTUAL_HOST=audioserve.nkontur.com
      - LETSENCRYPT_HOST=audioserve.nkontur.com
    command: /audiobooks
    volumes:
      - /mpool/audioserve/audiobooks:/audiobooks
      - /mpool/audioserve/data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O /dev/null http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  influxdb:
    container_name: influxdb
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Entrypoint chowns data dirs and switches to influxdb user
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETGID
      - SETUID
    ports:
      - "443:8086"
    networks:
      internal:
        ipv4_address: {{ influxdb_ip }}
    image: influxdb:2.8.0@sha256:8e911da5f7b482230e61fe4bad9af0697d97a75e087f19f5f0ddfee62c2bd686
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 4G
    volumes:
        - influxdb-storage:/var/lib/influxdb2
        - {{ docker_persistent_data_path }}/influxdb/config:/etc/influxdb2
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD={{ vault_influxdb_password | default(lookup('env', 'INFLUXDB_PASSWORD')) | trim | replace("$", "$$") }}
      - DOCKER_INFLUXDB_INIT_ORG=homelab
      - DOCKER_INFLUXDB_INIT_BUCKET=metrics
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN={{ vault_influxdb_admin_token | default(lookup('env', 'INFLUXDB_ADMIN_TOKEN')) | trim | replace("$", "$$") }}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8086/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  grafana:
    container_name: grafana
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: grafana/grafana:12.4.0-21693836646-ubuntu@sha256:ba93c9d192e58b23e064c7f501d453426ccf4a85065bf25b705ab1e98602bfb1
    ports:
      - "443:3000"
    networks:
      - internal
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G
    volumes:
      - grafana-storage:/var/lib/grafana
      - {{ docker_persistent_data_path }}/grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD={{ vault_grafana_admin_password | default(lookup('env', 'GRAFANA_ADMIN_PASSWORD')) | trim | replace("$", "$$") }}
      - GF_SERVER_ROOT_URL=https://grafana.lab.nkontur.com
      - GF_USERS_ALLOW_SIGN_UP=false
      - INFLUXDB_TOKEN={{ vault_influxdb_grafana_read_token | default(vault_influxdb_admin_token) | default(lookup('env', 'INFLUXDB_ADMIN_TOKEN')) | trim | replace("$", "$$") }}
      # SMTP for email alerts
      - GF_SMTP_ENABLED=true
      - GF_SMTP_HOST=smtp.gmail.com:587
      - GF_SMTP_USER=konoahko@gmail.com
      - GF_SMTP_PASSWORD={{ vault_grafana_smtp_password | default(lookup('env', 'GRAFANA_SMTP_PASSWORD')) | trim | replace("$", "$$") }}
      - GF_SMTP_FROM_ADDRESS=konoahko@gmail.com
      - GF_SMTP_FROM_NAME=Grafana Homelab
      - GF_SMTP_STARTTLS_POLICY=MandatoryStartTLS
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  loki:
    container_name: loki
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: grafana/loki:3.6.7@sha256:3c8fd3570dd9219951a60d3f919c7f31923d10baee578b77bc26c4a0b32d092d
    networks:
      loki-backend:
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
    volumes:
      - {{ docker_persistent_data_path }}/loki/data:/loki
      - {{ docker_persistent_data_path }}/loki/local-config.yaml:/etc/loki/local-config.yaml:ro
    command: -config.file=/etc/loki/local-config.yaml
    # Loki uses default json-file logging to avoid circular dependency
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "/usr/bin/loki", "-health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
  promtail:
    image: grafana/promtail:3.5.8@sha256:2a7c5469d687377de5cb7c8356cf96090c0069814d90ef35f9874db445999609
    container_name: promtail
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - DAC_READ_SEARCH
    restart: unless-stopped
    volumes:
      - /var/log:/var/log:ro
      - /etc/machine-id:/etc/machine-id:ro
      - {{ docker_persistent_data_path }}/promtail:/tmp/positions
      - {{ docker_persistent_data_path }}/promtail/config.yml:/etc/promtail/config.yml:ro
      - {{ docker_persistent_data_path }}/vault/logs:/vault/logs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - {{ docker_persistent_data_path }}/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    extra_hosts:
      - "loki.lab.nkontur.com:{{ lab_nginx_ip }}"
    networks:
      internal:
        ipv4_address: {{ promtail_ip }}
    mem_limit: 128m
    cpus: 0.25
    # Use json-file logging to avoid circular dependency (same as Loki)
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "grep -q ':2378' /proc/net/tcp6 2>/dev/null || grep -q ':2378' /proc/net/tcp"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
  gitlab:
    container_name: gitlab
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Omnibus GitLab runs ~10 internal services (postgres, redis, nginx, puma, etc.)
    # each as separate users, requiring broad privilege management
    cap_add:
      - CHOWN            # data dir ownership across service users (git, postgres, redis)
      - DAC_OVERRIDE     # cross-user file access for internal services
      - FOWNER           # chmod/chown across service users
      - FSETID           # preserve setgid bits on shared directories
      - KILL             # signal management for internal service supervision (runsvdir)
      - NET_BIND_SERVICE # bind to ports 80 and 22
      - SETGID           # switch between internal service groups
      - SETUID           # switch between internal service users
      - SYS_CHROOT       # PostgreSQL and other services use chroot
      - SYS_RESOURCE     # set ulimits for internal services (runsvdir-start)
    ports:
      - "22/tcp"
      - "443:80"
    networks:
      - internal
    image: 'gitlab/gitlab-ee:18.9.0-ee.0@sha256:280be239e059a6b841cdc31e40427ab393d5f23ec55ef36f58f26f30513fc8e0'
    restart: unless-stopped
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      nproc: 65536
    deploy:
      resources:
        limits:
          memory: 14G
        reservations:
          memory: 6G
    volumes:
      - '{{ docker_persistent_data_path }}/gitlab/config:/etc/gitlab'
      - '{{ docker_persistent_data_path }}/gitlab/logs:/var/log/gitlab'
      - '{{ docker_persistent_data_path }}/gitlab/data:/var/opt/gitlab'
      - '{{ docker_persistent_data_path }}/gitlab/license_encryption_key.pub:/opt/gitlab/embedded/service/gitlab-rails/.license_encryption_key.pub:ro'
    environment:
      MALLOC_CONF: 'dirty_decay_ms:1000,muzzy_decay_ms:1000'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/-/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 300s

  # HashiCorp Vault - Secrets Management
  # Initial setup uses file storage; will be configured for GitLab JWT auth in future MR
  vault:
    container_name: vault
    image: hashicorp/vault:1.21.3@sha256:5f244d447c6f90107149c9565da5ffedf847cec673ba0062a82fc3dd83c89e65
    restart: unless-stopped
    networks:
      internal:
        ipv4_address: {{ vault_ip }}
    ports:
      - "8200:8200"
    deploy:
      resources:
        limits:
          memory: 768M
    cap_drop:
      - ALL
    cap_add:
      - IPC_LOCK    # Lock memory to prevent secrets from being swapped
      - CHOWN       # File ownership changes during startup
      - DAC_OVERRIDE # Access files regardless of permissions
      - FOWNER      # Modify file ownership metadata
      - SETUID      # Process UID changes
      - SETGID      # Process GID changes
    security_opt:
      - no-new-privileges:true
    volumes:
      - vault_data:/vault/file
      - '{{ docker_persistent_data_path }}/vault/config:/vault/config:ro'
      - '{{ docker_persistent_data_path }}/certs:/vault/certs:ro'
      - '{{ docker_persistent_data_path }}/vault/scripts/auto-unseal.sh:/vault/scripts/auto-unseal.sh:ro'
      - '{{ docker_persistent_data_path }}/vault/unseal:/vault/unseal:ro'
      - '{{ docker_persistent_data_path }}/vault/logs:/vault/logs'
    environment:
      - VAULT_ADDR=https://127.0.0.1:8200
      - VAULT_API_ADDR=https://{{ vault_ip }}:8200
      - VAULT_CACERT=/vault/certs/nkontur.com/live/iot.lab.nkontur.com-0003/chain.pem
      # Skip TLS verify for healthcheck using loopback
      - VAULT_SKIP_VERIFY=true
      # Path to unseal keys file inside the container
      - VAULT_UNSEAL_KEYS_FILE=/vault/unseal/unseal-keys
    entrypoint: /bin/sh
    command: ["/vault/scripts/auto-unseal.sh"]
    healthcheck:
      test: ["CMD", "sh", "-c", "vault status -address=https://127.0.0.1:8200; [ $$? -le 2 ]"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # JIT Approval Service - Credential brokering with Telegram approval flow
  # Receives credential requests from openclaw, sends approval prompts via Telegram,
  # and issues short-lived Vault tokens on approval.
  jit-approval-svc:
    image: gitlab-registry.lab.nkontur.com:443/root/homelab/jit-approval-svc:latest
    container_name: jit-approval-svc
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    read_only: true
    cap_drop:
      - ALL
    tmpfs:
      - /tmp
    networks:
      internal:
        ipv4_address: {{ jit_approval_svc_ip }}
      jit-bridge:
    deploy:
      resources:
        limits:
          memory: 128M
    environment:
      - VAULT_ADDR=https://vault.lab.nkontur.com:8200
      - VAULT_ROLE_ID={{ vault_jit_approle_role_id | replace("$", "$$") }}
      - VAULT_SECRET_ID={{ vault_jit_approle_secret_id | replace("$", "$$") }}
      - TELEGRAM_BOT_TOKEN={{ vault_jit_telegram_bot_token | replace("$", "$$") }}
      - TELEGRAM_CHAT_ID=8531859108
      - TELEGRAM_WEBHOOK_SECRET={{ vault_jit_telegram_webhook_secret | replace("$", "$$") }}
      - LISTEN_ADDR=:8080
      - REQUEST_TIMEOUT=300
      - ALLOWED_REQUESTERS=prometheus
      - JIT_API_KEY={{ vault_jit_api_key | replace("$", "$$") }}
      - GITLAB_URL=https://gitlab.lab.nkontur.com
      - GITLAB_ADMIN_TOKEN={{ vault_jit_gitlab_admin_token | replace("$", "$$") }}
      - SSH_VAULT_PATH=ssh-client-signer
      - TAILSCALE_API_URL=https://api.tailscale.com
      - TELEGRAM_WEBHOOK_URL=https://jit-webhook.nkontur.com/telegram/webhook
      - PAPERLESS_URL=https://paperless-ngx.lab.nkontur.com
    depends_on:
      vault:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  deluge:
    container_name: deluge
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    ports:
      - "443:8112"
    image: 'lscr.io/linuxserver/deluge:2.0.3-2201906121747ubuntu18.04.1-ls107@sha256:4ddb60af86c6c17013a0ab143af9a1dcd2fcd517129a94d0b71e581bbe4f7937'
    restart: unless-stopped
    networks:
      internal:
        ipv4_address: {{ deluge_ip }}
    # Memory limit set to 2G after reducing libtorrent disk cache.
    # Default cache_size (512 pieces) causes 28GB+ RAM usage with large torrents.
    # Reduced to 128 pieces via core.conf — see configure-docker ansible role.
    deploy:
      resources:
        limits:
          memory: 2G
    volumes:
      - '{{ docker_persistent_data_path }}/deluge:/config'
      - '/mpool/samba_share/nfs:/downloads'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8112"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  bitwarden:
    container_name: bitwarden
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ports:
      - "443:80"
    image: 'vaultwarden/server:1.35.4-alpine@sha256:34b4e91ed4c2a4d2191245325608b2a84bedbeb9c9dd561e3c0924b98ef7126c'
    restart: unless-stopped
    networks:
      - external
    deploy:
      resources:
        limits:
          memory: 512M
    volumes:
      - '{{ docker_persistent_data_path }}/bitwarden/data/:/data/'
    env_file:
      - '{{ docker_persistent_data_path }}/bitwarden/global.override.env'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/alive"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  blog:
    image: wordpress:6.9.1-php8.3-apache@sha256:0324d403512533271f85831518d0d8618ae794f8d50e40103cbac473119188af
    container_name: wordpress
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ports:
      - "443:80"
    networks:
      - external
      - wordpress_internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      WORDPRESS_DB_HOST: wordpress_db
      WORDPRESS_DB_USER: wordpress
      WORDPRESS_DB_PASSWORD: {{ vault_wordpress_db_password | default(lookup('env', 'WORDPRESS_DB_PASSWORD')) | trim | replace("$", "$$") }}
      WORDPRESS_DB_NAME: wordpress
    volumes:
      - {{ docker_persistent_data_path }}/wordpress/html:/var/www/html
      - {{ docker_persistent_data_path }}/wordpress/config:/etc/wordpress
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  wordpress_db:
    image: mysql:9.6.0-oraclelinux9@sha256:db32c8ec843c042a728efb0ac7aa814d6f010eaac8923e20ae0a849d09c5baf8
    container_name: wordpress_db
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    networks:
      - wordpress_internal
    restart: unless-stopped
    # MySQL entrypoint chowns data dir and drops to mysql user
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETGID
      - SETUID
      - SYS_NICE         # MySQL IO thread scheduling priority
    deploy:
      resources:
        limits:
          memory: 768M
    environment:
      MYSQL_DATABASE: wordpress
      MYSQL_USER: wordpress
      MYSQL_PASSWORD: {{ vault_wordpress_db_password | default(lookup('env', 'WORDPRESS_DB_PASSWORD')) | trim | replace("$", "$$") }}
      MYSQL_ROOT_PASSWORD: {{ vault_wordpress_db_password | default(lookup('env', 'WORDPRESS_DB_PASSWORD')) | trim | replace("$", "$$") }}
    volumes:
      - wordpress_db:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  diagram:
    container_name: diagram
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    ports:
      - "443:8080"
    networks:
      internal:
    image: jgraph/drawio:29.5.2@sha256:5ef8f7bd04e2b6d059f21312e67e816df33ae7e404fd2fa50facbdf41b4ec8ef
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 768M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  homeassistant:
    container_name: homeassistant
    networks:
      - internal
      - iot
      - external
    ports:
      - "443:8123"
    devices: 
      - "/dev/serial/by-id/usb-Prolific_Technology_Inc._USB-Serial_Controller_ETDRb11A920-if00-port0:/dev/ttyUSB0"
      - "/dev/serial/by-id/usb-Prolific_Technology_Inc._USB-Serial_Controller-if00-port0:/dev/ttyUSB1"
    image: "homeassistant/home-assistant:2026.2.3@sha256:96fa92d83fa8dae987fbbbcf58b1fea1140985ff6a8517b37f7b65c76ef20133"
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    volumes:
      - "{{ docker_persistent_data_path }}/homeassistant:/config"
      - /etc/localtime:/etc/localtime:ro
    command: ["/bin/bash", "-c", "ip route del default; ip route add default via 10.3.0.1 dev eth1; /init"]
    cap_drop:
      - ALL
    cap_add:
      # Required: the entrypoint command runs `ip route del default; ip route add default via 10.3.0.1 dev eth1`
      # to override the default route so HA traffic goes through the internal VLAN (10.3.x.x) instead of
      # whatever Docker assigns. Without NET_ADMIN, the ip route commands fail and HA gets wrong default gateway.
      - NET_ADMIN
    security_opt:
      - no-new-privileges:true
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -s -o /dev/null http://localhost:8123/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
  snapserver:
    container_name: snapserver
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    hostname: snapserver
    networks:
      iot:
        ipv4_address: {{ snapserver_address }}
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapcast:latest"
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - "{{ docker_persistent_data_path }}/snapserver/snapserver.conf:/etc/snapserver.conf"
      - "{{ docker_persistent_data_path }}/snapserver/server.json:/root/.config/snapserver/server.json"
      - "/tmp/snapfifo:/tmp/snapfifo"
      - "fifos:/tmp/fifo"
    restart: unless-stopped
    devices:
      - "/dev/snd"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:1780"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  snapclient_office:
    container_name: snapclient_office
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Audio clients need user switching and real-time scheduling for playback
    cap_add:
      - SETGID    # drop to audio group
      - SETUID    # drop to audio user
      - SYS_NICE  # real-time audio scheduling priority
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Office,DEV=0"
      - "--hostID"
      - "office"
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/status && head -1 /proc/1/status | grep -q snapclient"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_global:
    container_name: snapclient_global
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID         # drop to audio group
      - SETUID         # drop to audio user
      - SYS_NICE       # real-time audio scheduling priority
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Global,DEV=0"
      - "--hostID"
      - "global"
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/status && head -1 /proc/1/status | grep -q snapclient"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_kitchen:
    container_name: snapclient_kitchen
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as snapclient_office
    cap_add:
      - SETGID
      - SETUID
      - SYS_NICE
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Kitchen,DEV=0"
      - "--hostID"
      - "kitchen"
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/status && head -1 /proc/1/status | grep -q snapclient"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_main_bedroom:
    container_name: snapclient_main_bedroom
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as snapclient_office
    cap_add:
      - SETGID
      - SETUID
      - SYS_NICE
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Main_Bedroom,DEV=0"
      - "--hostID"
      - "main_bedroom"
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/status && head -1 /proc/1/status | grep -q snapclient"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_main_bathroom:
    container_name: snapclient_main_bathroom
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as snapclient_office
    cap_add:
      - SETGID
      - SETUID
      - SYS_NICE
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Main_Bathroom,DEV=0"
      - "--hostID"
      - "main_bathroom"
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/status && head -1 /proc/1/status | grep -q snapclient"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_guest_bathroom:
    container_name: snapclient_guest_bathroom
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as snapclient_office
    cap_add:
      - SETGID
      - SETUID
      - SYS_NICE
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Guest_Bathroom,DEV=0"
      - "--hostID"
      - "guest_bathroom"
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/status && head -1 /proc/1/status | grep -q snapclient"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  snapclient_guest_bedroom:
    container_name: snapclient_guest_bedroom
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Same as snapclient_office
    cap_add:
      - SETGID
      - SETUID
      - SYS_NICE
    networks:
      iot:
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/snapclient:latest"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
    devices:
      - "/dev/snd"
    command:
      - "-h"
      - {{ snapserver_address }}
      - "-s"
      - "plughw:CARD=Guest_Bedroom,DEV=0"
      - "--hostID"
      - "guest_bedroom"
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/status && head -1 /proc/1/status | grep -q snapclient"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
  # Private registry container removed - using GitLab's built-in registry instead
  # Images now pulled from: gitlab-registry.lab.nkontur.com:443/root/homelab/<image>
  mosquitto:
    container_name: mosquitto
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # Entrypoint chowns data/config and drops to mosquitto user
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETGID
      - SETUID
    read_only: true
    tmpfs:
      - /tmp
    image: eclipse-mosquitto:2.1.2-alpine@sha256:9cfdd46ad59f3e3e5f592f6baf57ab23e1ad00605509d0f5c1e9b179c5314d87
    restart: unless-stopped
    networks:
      iot:
        ipv4_address: {{ mosquitto_address }}
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - "{{ docker_persistent_data_path }}/mqtt/conf:/mosquitto/config"
      - mosquitto:/mosquitto/data
      - "{{ docker_persistent_data_path }}/certs:/mosquitto/certs"
    healthcheck:
      test: ["CMD-SHELL", "echo | nc -w1 127.0.0.1 1883 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  amcrest2mqtt:
    container_name: amcrest2mqtt
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    image: "gitlab-registry.lab.nkontur.com:443/root/homelab/amcrest2mqtt:latest"
    depends_on:
      mosquitto:
        condition: service_healthy
    restart: unless-stopped
    networks:
      iot:
    deploy:
      resources:
        limits:
          memory: 256M
    environment:
      AMCREST_HOST: 10.6.128.9
      AMCREST_PASSWORD: {{ vault_doorbell_pass | default(lookup('env', 'DOORBELL_PASS')) | trim | replace("$", "$$") }}
      MQTT_HOST: "mqtt.lab.nkontur.com"
      MQTT_PORT: "8883"
      MQTT_USERNAME: mosquitto
      MQTT_PASSWORD: {{ vault_mqtt_pass | default(lookup('env', 'MQTT_PASS')) | trim | replace("$", "$$") }}
      HOME_ASSISTANT: "true"
      MQTT_TLS_ENABLED: "true"
      MQTT_TLS_CA_CERT: "/etc/ssl/certs/ca-certificates.crt"
      STORAGE_POLL_INTERVAL: 0
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f amcrest2mqtt || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  ambientweather:
    image: ghcr.io/neilenns/ambientweather2mqtt:latest@sha256:a9d9899ebf9c2af9378c275e83ead6ced841fe9d051a6aec4e10e25eb5193e21
    restart: unless-stopped
    container_name: ambientweather
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    networks:
      iot:
        ipv4_address: {{ weather_mqtt_bridge_ip }}
    deploy:
      resources:
        limits:
          memory: 256M
    environment:
      STATION_MAC_ADDRESS: {{ weather_station_mac }}
      MQTT_SERVER: "mqtts://mqtt.lab.nkontur.com:8883"
      TZ: America/New_York
      PORT: 8080
      MQTT_USERNAME: mosquitto
      MQTT_PASSWORD: {{ vault_mqtt_pass | default(lookup('env', 'MQTT_PASS')) | trim | replace("$", "$$") }}
      MQTT_REJECT_UNAUTHORIZED: "false"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/data/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  zigbee2mqtt:
    container_name: zigbee2mqtt
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    restart: unless-stopped
    image: koenkk/zigbee2mqtt:2.8.0@sha256:89cf02f379aa743a68494388e3a26fba7b8c9101f8b452038cc07aeff3fc983c
    depends_on:
      mosquitto:
        condition: service_healthy
    networks:
      iot:
    ports:
      - 8081:8080
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - "{{ docker_persistent_data_path }}/zigbee2mqtt:/app/data"
    environment:
      - TZ=America/New_York
    healthcheck:
      test: ["CMD", "wget", "--spider", "--quiet", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  paperless-ngx:
    image: lscr.io/linuxserver/paperless-ngx:1.7.1@sha256:4016f13609923d1ef926c3d3532cf77f3650dc0b886258947cdf311c5ddc9829
    container_name: paperless-ngx
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID
      - SETGID
      - CHOWN
      - DAC_OVERRIDE
    networks:
      internal:
    deploy:
      resources:
        limits:
          memory: 768M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
      - PAPERLESS_URL=https://paperless-ngx.lab.nkontur.com
    volumes:
      - "{{ docker_persistent_data_path }}/paperless:/config"
      - "/mpool/nextcloud/paperless:/data"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  mopidy:
    image: wernight/mopidy:latest@sha256:e3156f3da69d3e88b191c852d1fa60907c91e36668bca1f5afb5b57b74a8407d
    container_name: mopidy
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    networks:
      iot:
        ipv4_address: {{ mopidy_address }}
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - TZ=America/New_York
    volumes:
      - "{{ docker_persistent_data_path }}/mopidy/media:/var/lib/mopidy/media:ro"
      - "{{ docker_persistent_data_path }}/mopidy/local:/var/lib/mopidy/local"
      - "{{ docker_persistent_data_path }}/mopidy/playlists:/var/lib/mopidy/playlists"
      - "{{ docker_persistent_data_path }}/mopidy/config/mopidy.conf:/config/mopidy.conf"
      - "fifos:/tmp/fifo"
    devices:
      - "/dev/snd"
    ports:
      - "443:6680"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6680"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  nextcloud_db:
    image: mariadb:12.2.2
    container_name: nextcloud_database
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # MariaDB entrypoint chowns data dir and drops to mysql user
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETGID
      - SETUID
    command: --transaction-isolation=READ-COMMITTED --binlog-format=ROW
    restart: unless-stopped
    networks:
      - nextcloud_internal
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 2G
    volumes:
      - nextcloud_db:/var/lib/mysql
    environment:
      - MYSQL_ROOT_PASSWORD={{ vault_nextcloud_db_password | default(lookup('env', 'NEXTCLOUD_DB_PASSWORD')) | trim | replace("$", "$$") }}
      - MYSQL_PASSWORD={{ vault_nextcloud_db_password | default(lookup('env', 'NEXTCLOUD_DB_PASSWORD')) | trim | replace("$", "$$") }}
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  nextcloud:
    image: nextcloud:32.0.6-apache@sha256:0e1084cc59df77bec7d6bb29d9ac6939da8372512237a9c51f74ff0a970524f2
    container_name: nextcloud
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - CHOWN
      - FOWNER
      - DAC_OVERRIDE
    networks:
      - external
      - nextcloud_internal
    hostname: nkontur.com
    ports:
      - "443:80"
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 2G
    volumes:
      - nextcloud:/data
      - {{ docker_persistent_data_path }}/nextcloud/config.php:/var/www/html/config/config.php
      - /etc/localtime:/etc/localtime:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/status.php"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  prowlarr:
    image: lscr.io/linuxserver/prowlarr:latest@sha256:e74a1e093dcc223d671d4b7061e2b4946f1989a4d3059654ff4e623b731c9134
    container_name: prowlarr
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    restart: unless-stopped
    ports:
      - "443:9696"
    networks:
      - internal
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/prowlarr:/config
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9696/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  navidrome:
    image: deluan/navidrome:latest@sha256:a5dce8f33304714dd138e870cca0dcab3d937ca236be1a9f2b97da009d1a0048
    container_name: navidrome
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    networks:
      - internal
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 128M
    environment:
      - ND_SCANSCHEDULE=1h
      - ND_LOGLEVEL=info
      - ND_SESSIONTIMEOUT=24h
      - ND_BASEURL=/
    volumes:
      - {{ docker_persistent_data_path }}/navidrome:/data
      - /mpool/plex/Music:/music:ro
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:4533/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
  lidarr:
    image: lscr.io/linuxserver/lidarr:latest@sha256:37a3df74f4c2a6f10eead66f4d8034362ebf2866f935026b4a71dd888b9e7f08
    container_name: lidarr
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETUID         # s6-overlay drops to PUID/configured user
      - SETGID         # s6-overlay drops to PGID/configured group
      - CHOWN          # s6-overlay chowns data directories at startup
      - DAC_OVERRIDE   # s6-overlay reads/writes files across users during init
    networks:
      - internal
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/New_York
    volumes:
      - {{ docker_persistent_data_path }}/lidarr:/config
      - /mpool/plex/Music:/music
      - /mpool/samba_share/nfs:/remote
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8686/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
  chromium-browser:
    build:
      context: "{{ lookup('env', 'CI_PROJECT_DIR') + '/docker/chromium-browser' }}"
    image: gitlab-registry.lab.nkontur.com:443/root/homelab/chromium-browser:latest
    container_name: chromium-browser
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SYS_ADMIN
      - CHOWN
      - SETUID
      - SETGID
      - FOWNER
      - DAC_OVERRIDE
    networks:
      internal:
        ipv4_address: "{{ chromium_browser_ip }}"
    deploy:
      resources:
        limits:
          memory: 2G
    volumes:
      - "{{ docker_persistent_data_path }}/chromium-browser/profile:/data/chrome-profile"
      - "{{ docker_persistent_data_path }}/chromium-browser/uploads:/uploads:ro"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9222/json/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    environment:
      - TZ=America/New_York
      - VNC_PASSWORD={{ vault_chromium_vnc_password | default('changeme') | replace("$", "$$") }}
  openclaw-gateway:
    image: gitlab-registry.lab.nkontur.com:443/root/homelab/openclaw:{{ openclaw_image_tag | default('latest') }}
    container_name: openclaw-gateway
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    restart: unless-stopped
    networks:
      internal:
      iot:
      mgmt:
        ipv4_address: {{ openclaw_gateway_ip }}
      openclaw-nginx-bridge:
    extra_hosts:
      - "nkontur.com:172.30.0.10"
      - "www.nkontur.com:172.30.0.10"
    ports:
      - "18789:18789"
    deploy:
      resources:
        limits:
          memory: 3G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18789/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    environment:
      - TZ=America/New_York
      # Core openclaw configuration
      - OPENAI_API_KEY={{ vault_openai_api_key | default(lookup('env', 'OPENAI_API_KEY')) | trim | replace("$", "$$") }}
      - CLAWDBOT_GATEWAY_TOKEN={{ vault_moltbot_gateway_token | default(lookup('env', 'MOLTBOT_GATEWAY_TOKEN')) | trim | replace("$", "$$") }}
      - TELEGRAM_BOT_TOKEN={{ vault_moltbot_telegram_token | default(lookup('env', 'MOLTBOT_TELEGRAM_TOKEN')) | trim | replace("$", "$$") }}
      - GITLAB_TOKEN={{ vault_moltbot_gitlab_token | default(lookup('env', 'MOLTBOT_GITLAB_TOKEN')) | trim | replace("$", "$$") }}
      # IPMI credentials removed — use JIT T2 (ipmi resource) instead
      # Web search
      - BRAVE_API_KEY={{ brave_api_key }}
      # Agent platforms
      - ACLAWDEMY_API_KEY={{ vault_aclawdemy_api_key | default(lookup('env', 'ACLAWDEMY_API_KEY')) | trim | replace("$", "$$") }}
      # Vault AppRole credentials (for openclaw runtime Vault access & JIT)
      - VAULT_ADDR=https://vault.lab.nkontur.com:8200
      - VAULT_APPROLE_ROLE_ID={{ lookup('env', 'VAULT_APPROLE_ROLE_ID') }}
      - VAULT_APPROLE_SECRET_ID={{ lookup('env', 'VAULT_APPROLE_SECRET_ID') }}
    volumes:
      - {{ docker_persistent_data_path }}/openclaw/data:/home/node/.openclaw
      - {{ docker_persistent_data_path }}/openclaw/mcporter:/home/node/.mcporter
      - {{ docker_persistent_data_path }}/openclaw/workspace:/home/node/.openclaw/workspace
    command: ["node", "dist/index.js", "gateway", "--bind", "lan", "--port", "18789"]

  # =============================================================================
  # OpenStreetMap Stack — self-hosted mapping services
  # =============================================================================

  # Valhalla — routing engine (turn-by-turn directions, isochrones, matrix)
  # First startup downloads ~11GB full US OSM extract and builds routing tiles (several hours).
  # Subsequent starts are fast — tiles are cached in the data volume.
  valhalla:
    container_name: valhalla
    image: ghcr.io/valhalla/valhalla-scripted:3.6.3@sha256:27edb9e58564e0bf082c21e8c8460a0968dfa2b3892a9db3b1199afc3bee5937
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
      - DAC_OVERRIDE
      - FOWNER
    networks:
      - internal
    restart: unless-stopped
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 4G
    environment:
      - tile_urls=https://download.geofabrik.de/north-america/us-latest.osm.pbf
      - serve_tiles=True
      - build_elevation=True
      - build_admins=True
      - build_time_zones=True
      - force_rebuild=False
    volumes:
      - {{ docker_persistent_data_path }}/valhalla/custom_files:/custom_files
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 300s

  # TileServer GL — vector/raster map tile server
  # NOTE: You must download an .mbtiles file into the data volume before this works.
  # Options: https://data.maptiler.com/downloads/planet/ or generate with openmaptiles.
  # Place .mbtiles files in {{ docker_persistent_data_path }}/tileserver/
  tileserver:
    container_name: tileserver
    image: maptiler/tileserver-gl:latest@sha256:0472aa881c9965bcd578cbaaeeaf5ab86d5d92e10c64c48c6856a6b1b7beb13f
    # Run as UID 1000 to match tileserver-gl's expected user.
    # The deploy-compose role pre-creates the data dir with 1000:1000 ownership.
    user: "1000:1000"
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    networks:
      - internal
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 256M
    volumes:
      - {{ docker_persistent_data_path }}/tileserver:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Nominatim — geocoding / reverse-geocoding engine
  # Uses US-Midwest regional extract to keep import fast and memory reasonable.
  # First startup imports the PBF extract into PostgreSQL (can take 1-2 hours).
  nominatim:
    container_name: nominatim
    image: mediagis/nominatim:5.2.0-2025-12-04T14-31@sha256:3c49ad9443baab1f1ea13a6b1355fa377ae5fb0874dc328cba9b97a0ca7914bb
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
      - DAC_OVERRIDE
      - FOWNER
    networks:
      - internal
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 6G
    environment:
      - PBF_URL=https://download.geofabrik.de/north-america/us-midwest-latest.osm.pbf
      - REPLICATION_URL=https://download.geofabrik.de/north-america/us-midwest-updates/
      - NOMINATIM_PASSWORD={{ vault_nominatim_password | default(lookup('env', 'NOMINATIM_PASSWORD')) | trim | replace("$", "$$") }}
      - IMPORT_STYLE=extratags
    volumes:
      - {{ docker_persistent_data_path }}/nominatim/data:/var/lib/postgresql/16/main
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      # Initial PBF import can take 1-2 hours; 10min was too short causing restart loops
      start_period: 7200s
